\documentclass[b5paper]{report}
\usepackage[lmargin=25mm,rmargin=25mm,tmargin=27mm,bmargin=30mm]{geometry}

\usepackage[toc]{appendix}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{color}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{parskip}
\usetikzlibrary{positioning,shapes,decorations,calc,fit,arrows.meta}

\usepackage{libertine}
\usepackage{inconsolata}
\usepackage{libertinust1math}
\usepackage[T1]{fontenc}
\catcode`\_=12

\begin{document}
\title{Concurrent Memory Reclamation on ARM}
\author{Martin Hafskjold Thoresen}
\date{\today}
\maketitle

\input{macros.tex}

\begin{abstract}
  \todo{}
\end{abstract}

\tableofcontents

\chapter{Introduction}
\todo{Add note on course at NTNU in the fall of 2017 etc?}
% \note{Outline: parallelism, garbage collection, what we are about to do,
% rust, performance, usability}

In 1965 Gordon Moore stated what is now knows as \emph{Moore's Law}, which
roughly states that the number of transistors on integrated circuits will double
every 18 months. The law has also been used to describe the increse in clock
speed of central processing units (CPU's). During the 2000s however, this growth
slowed drastically, and CPU's sold today have very similar clock speeds as those
sold in 2005. Desipte the clock speed stagnation, there have been major
performance improvements in the last 15 years. We have more cache, improved
branch predictors, larger vector instructions, superscalar architechtures, and
\emph{multi-core systems}, the latter of which is central to this report.
Multi-core systems are now ubiquitous; high end desktop CPUs with 16 cores are
available, and even smartphones with 8 cores are in the market in 2017.

The introduction and growth of multi-core systems, however, has not been as
noticeable as the former improvements in clock speed. One reason for this is
that many real world problems and systems are inherently serial problems. Due to
Amdahls Law, this even puts an upper bound on the possible speedup we can get by
using concurrency.  Another reason is that programs already written cannot
automatically use the extra parallelism that newer architectures give us;
parallelism needs to be accounted for by programmers, and often imply a major
shift in program architecture. \todo{?}A third reason may be that concurrent systems are
extremely complex. Bugs are (usually) not reproducible, tests are not
deterministic, and the combinatorial explosion of possible interwindings of
executions across threads makes a brute force approach to program verification
intractable.

\todo{want to say that the complexity of concurrency means that most programmers
have already given it up, due to the lack of good, generic contstructs for eg.
work sharing, thread management, synchronization, and communication.}
A consequence of the complexity of concurrent systems is that most programming
languages have poor support for concurrency, as it is often regarded as a last
resort, or an area reserved for expert programmers only.
In short, concurrency has yet to be approachable for a lot of programmers.

In this report we look at the problem of concurrent memory reclamation. The
problem can be informally stated as ``how to we know when it is safe to free
allocated memory in a concurrent system?''. In Chapter~\ref{ch:background} we
look at the needed prerequisites in order to fully understand the problem and
its nuances; in Chapter~\ref{ch:memory-reclamation} we look at different memory
reclamation schemes, both the ones implemented here and a highlight of newer
variants; in Chapter~\ref{ch:methodology} we look at the practical side of this
project: implementing memory reclamation schemes in Rust, as well as testing and
benchmarking methods; in Chapter~\ref{ch:results} we look at the experimental
results from running the system implemented; and in Chapter~\ref{ch:discussion}
we summarize the project, comment on methodologies, and suggest future work.

\note{what is the motivation behind this? Do we want to make the claim that Rust
is making low level stuff more approachable, and that we can help improving the
performance of programs written by having CMR in Rust?}

\note{Parallelism isn't a success story for programmers. Hard, error-prone, even
simple things goes wrong. Need effective abstractions to improve this.??}


\chapter{Background\label{ch:background}}

\todo{Consider adding MS queue or something to show pitfalls in concurrent programming?}

The field of computer science is broad and diverse, so readers may have very
different prerequisites.  We will assume the reader is familiar with traditional
memory management (as in C), and the basics of computer hardware, such as the
existence of cahce memory. The report will (hopefully) be approachable to anyone
with this background.

\section{Terminology} We start with basic terminology of memory management. Most
of the programs we write \emph{allocate} memory, meaning requesting a memory
range from the OS for exclusive use by the program. This memory is then
\emph{deallocated}, or \emph{freed}, when we are done with it, so that the
memory may be used at a later time for a different purpouse; the memory is thus
\emph{reclaimed}. If we want to ``mark'' memory as freed without freeing it
(that is, the program still has exclusive use of it) we say we \emph{retire} the
memory (reasons for doing so will become apparent). In concurrent settings, we
risk accessing memory at the same time; a \emph{data race}, or a \emph{race
condition} is when multiple threads are operating on the same memory
simultaneously, and where at least one operation is a write.

Both modern compilers and CPUs may reorder program instructions if they think it
will improve performance, for instance by improving memory locality and thus
improving cache behavior, or reduce pipeline stalling. This may or may not be
acceptable by the program. The guarantee that the instructions executed appear
to take effect in program order is called \emph{sequential consistency} (SC),
and was defined by Leslie Lamport in 1979\todo{source here}. We call a system in
which threads can be prevented from making progress \emph{blocking}. The
opposite of blocking is \emph{non-blocking}. A system is \emph{lock-free} if we
have a guarantee that \emph{some} thread will make progress. Note that any given
thread may be blocked for an infinite amount of time. The guarantee that
\emph{any} given thread will eventually make progress is called
\emph{wait-freedom}.  For a more thorough introduction
see~\cite{herlihy2011art}.

In communities dealing with concurrency and parallelism the meaning of the two
words is often debated. We will settle for the informal definitions given
in~\cite{pacheco2011introduction}. An informal distinction is that in a parallel
system multiple tasks run simultaneously, while in a concurrent system multiple
tasks may be in progress at the same time.


\section{Rust}\label{sec:rust}

Rust~\cite{rust} is a programming language which focus is safety, performance,
and concurrency. It is is freely developed by over 1900 contributors on the
version control platform GitHub~\cite{github}, and is officially sponsored by
Mozilla. Rust 1.0 was released in May 2015, and the current stable version of
the language is 1.22, with a new version released every 6th week. The language
is compiled and typed, and includes features such as type inference, pattern
matching, tagged enums, template-like generics, and a minimal runtime without a
garbage collector. As of November 2017 the official Rust compiler \code{rustc}
uses LLVM~\cite{llvm} for optimization and code generation. The language has no
formal specification, and language changes are done through an RFC (``request
for comments'') process. The syntax of Rust is out of scope for this report, so
we will settle with a highlight of the most important features of Rust. For a
thorough introduction to Rust, see The Rust Programming Language\cite{trpl}.

As C++, Rust has structs. It does not however have classes or inheritance. The
type system is based around \emph{traits}, which are comparable to interfaces in
Java. A trait defines methods, optionally with an implementation. We can then
\emph{implement} a trait for a struct. Implementing a trait for a struct
requires that either the trait of the struct is defined in the \emph{crate} (the
project unit) the implementation is in. A consequence of this is that it is
possible to extend types from the standard library with ones own traits. It is
possible to have values which type is a trait, called \emph{trait objects}. This
is useful when we want to be agnostic of the implementation of an interface.
However, this requires dynamic dispatch of the function calls of the value,
which imposes a runtime overhead.

Despite the lack of a garbage collector, Rust programmers does not have to
manually manage memory. Rust solves this through \emph{ownership} semantics:
values are either \emph{owned} or \emph{borrowed} by its scope. Transferring
ownership of a value is possible by \emph{moving} it. When a borrowed value goes
out of scope, nothing happens as a borrowed value is simply a pointer. When an
owned value goes out of scope it is \emph{dropped}, meaning its destructor is
ran. Rust's ownership rules, which are enforced by the compiler, guarantees that
there are no other references to the object when it is dropped. Implementing the
destructor of a type is done by implementing the \code{Drop} trait. There is no
trait for constructors, but by convention a method called \code{new} is used.

An example of an abstraction using both \code{new} and \code{Drop} is
\code{std::box::Box<T>}, a fat pointer to a heap allocated value of type
\code{T}. The \code{new} implementation allocates the memory, and \code{drop}
will free the memory. This pattern is often called \emph{RAII} (resource
allocation is initialization). This way we get a safe abstraction over heap
allocation, since the memory is freed exactly once when the \code{Box} goes out
of scope.  \code{Box<T>} also implements another trait, \code{Deref<T>}, which
causes \code{Box} references to automatically be converted by the compiler to
\code{T} references, if needed. We note that we can \emph{not} pass a
\code{Box<T>} to a function that takes a \code{T}, since the two types have
different \code{drop} implementations.

In addition to the ownership rules, Rust have two important rules for
references: references are always valid, and mutable references are exclusive.
At compile time Rust tracks the \emph{lifetime} of all values, and ensures that
the lifetime of all references does not outlive the lifetime of the data they
are referencing. If a function returns a reference to a value allocated on the
stack of that function it is caught at compile time, since the reference's
lifetime would outlive the functions lifetime, and the values lifetime is at
most that of the function. The second rule stops one reference to invalidate the
data that another reference points to. For instance it is not allowed (and
caught at compile time) to have a reference to a value inside a \code{Vec} and
mutate the \code{Vec}, since mutating the \code{Vec} might cause the data to be
moved, which invalidates the reference.\todo{feedback on this paragraph}

\subsection{Unsafe Rust}

Sometimes the rules of Rust are too strict. Rust, aiming to be a pragmatic
language, offers an escape hatch for this: \emph{unsafe code}. In unsafe code it
is allowed to: dereference raw pointers, call \code{unsafe} functions, implement
\code{unsafe} traits, and mutate static variables. Raw pointers are, as in C and
C++, just a memory address, and as with references, both a immutable
(\code{*const T}) and mutable (\code{*mut T}) variants exist. The difference
between references and raw pointers is that raw poiters do not have lifetimes
attached to them. In unsafe Rust, we enter the dangerous and error-prone
territory that C and C++ programmers know all too well. Examples of
\code{unsafe} functions are all FFI functions, unchecked array indexing, and
creating a \code{String} without UTF-8 checks (all \code{String}s in Rust are
valid UTF-8). \code{unsafe} code is often more about signaling to the programmer
that the code is fragile, and that one should put extra care into making sure
that ones assumptions are correct. For instance, dereferencing raw pointers is
\code{unsafe}, but it may happen that we know that the pointer is still valid,
eg.\ if it points to heap memory that we explicitly leaked, using
\code{std::mem::forget}.

By exposing \code{unsafe} to programmers, one might argue that Rust is just as
unsafe as C or C++. The primary advantage of \code{unsafe} is that the ratio of
unsafe to safe code is very low. For instance, the Rust compiler itself consists
only of $1\%$ unsafe code\cite{rustc-unsafe}. This makes it possible to focus on
the few places in a code base that is unsafe when developing, testing, and code
reviewing. It is however important to realize that the effects of unsafe code is
non-local: bugs in unsafe code can cause undefined behavior to take effect in
perfectly safe code.



\section{Concurrency in Hardware \todo{HTM?}}

Most programmers rarely have to think about the hardware their programs run on.
Caches, for instance, is designed to be transparent for programmers. With
concurrency this changes, as concurrent programming have challenges in which a
mental model of how the hardware works is paramount. In this section we consider
multi-core CPU's with multiple levels of cache; this includes most modern CPU's,
both desktop computers and laptops, as well as most mobile devices, such as
smartphones and tablets.

The cache hierarchy poses a problem for shared memory programs. Each core
usually have their own L1 cache, while the remaining levels, L2 (and maybe L3),
are shared between cores. This means that when a program is writing to memory,
the same logical memory may be present in multiple physical locations on the
CPU.\@ This causes synchronization problems when this memory is written to. This
is the problem of \emph{cache coherence}. A memory range does not even have to
be used by multiple threads: since caches operate on memory chunks, called
\emph{cache lines}, it is enough that two values are on the same cache line for
synchronization problems to occur. This is called \emph{false sharing}.

We would like our programs to be sequentially consistent, as this maps the
program execution to the source code of the program. However, we also want our
programs to be performant, and it turns out that from a hardware perspective
these two requirements are conflicting. CPUs are simply not sequentially
consistent.

CPU architectures have rules on how much the CPU is allowed to reorder reads and
writes without breaking the semantics of the program when ran sequentially. They
also have instructions explicitly for avoiding instruction reordering, called
\emph{memory fences} (or \emph{barriers}; we will use the former name). The x86
architecture have a rather strong \emph{memory model}, meaning the CPU is very
limited in its ability to reorder.  For instance, x86 forbids reads to be
reordered with other reads, and writes with other writes. It also forbids any
reordering with locked instructions; this is very strict, as the preferred
instruction used for sequential consistent atomic store is a locked instruction,
meaning atomic stores are full fences. ARM on the other hand, have a comparably
weak memory model. A disadvantage of this is that sequentially consistent
atomics require full fences for both loads and stores, since they are allowed to
be reordered. ARMv8 offers a solution to this by having explicit instructions
for SC loads and stores. For more information on x86, see Chapter 8.2.2
in~\cite{intel64}; for information about ARMv7 see~\cite{armv7-reference-manual},
and for ARMv8 see~\cite{armv8-reference-manual}.

Many instruction sets have \emph{atomic} operations. An operation is atomic if
its is performed either fully or not at all. For instance, if thread A
non-atomically stores a value at a location, thread B may observe the write when
it is only partially performed. This is not possible using atomic instructions.
Atomic load and store is the two most fundamental atomic operations, but most
platforms also give us additional instructions; the most notable being
\code{compare-and-swap} (\code{CAS}) and \code{fetch-and-add} (\code{FAA}).
\code{CAS} takes three arguments: a memory address, a value A, and a value B. If
the value at the given address is A, it writes B to the address. If not, nothing
is done. \code{FAA} atomically increments a memory location by the specified
value. \code{CAS} is a central building block in concurrent programming, and
especially in lock-free programming.

\section{Concurrency in Software}

Modern operating systems run programs as \emph{processes}. Processes may spawn
other processes or \emph{threads} for concurrent program execution. Processes
and threads differ in that threads share the memory space of their parent
process, while processes do not. Since we are looking at memory reclamation, we
will only be looking at shared memory concurrency, meaning concurrency using
threads.

\todo{write about mutexes, semaphores, message passing, work stealing, condvars,
cilk, rayon, go, green threads}

We end this section with an example of instruction reordering. Consider
Listing~\ref{lst:reordering}, and assume variables are initialized to 0.  When
ran sequentially (first B and then A), we would observe no difference if the
assignments of thread B were reordered; the end result would be that we print
\code{1}. However, when run concurrently thread B might decide to assign \code{f
= 1} first, and thread A will risk printing \code{0} instead of \code{1}. This
outcome was impossible in the sequential world.
\begin{figure}[ht]
\begin{lstlisting}[caption=Instruction reordering,label=lst:reordering]
// Thread A                      // Thread B
while f == 0 { }                 x = 1
print(x)                         f = 1
\end{lstlisting}
\end{figure}

A solution to this problem is to use a memory fence, in order to explicitly
disallow any reordering past it, as in Listing~\ref{lst:mem-fence}.
\begin{figure}[ht]
\begin{lstlisting}[caption=Memory fence for synchronization,label=lst:mem-fence]
// Thread A                      // Thread B
while f == 0 { }                 x = 1
fence()                          fence()
print(x)                         f = 1
\end{lstlisting}
\end{figure}

LLVM defines a memory model which is inspired by the C++11 memory
model~\cite{llvmmm}. The model includes six \emph{memory ordering constraints},
which are used in atomic operations. The orderings dictates how the compiler and
the CPU is allowed to reorder the instructions around operations the orderings
are used on.  We look at five of them here: \code{monotonic}, \code{acquire},
\code{release}, \code{acq\_rel}, and \code{seq\_cst}.  \code{monotonic} is the
weakest and offers no ordering constraints. Atomic operations with this ordering
differs only from regular operations in that the operation cannot be observed to
happen partially.  \code{acquire} and \code{release} are intended to work in
pairs, by loading with \code{acquire} and storing with \code{release} in the
same memory location.  An \code{acquire} load of X ensures that subsequent loads
will see all stores that happend before a \code{release} store to X.  This can
be used to implement a lock, where we \code{acquire} the lock before the
critical section, and \code{release} the lock after it.  Now we get the
guarantee that two critical sections cannot overlap, since the stores before the
\code{realease} in the first section must be visible upon the \code{acquire}
load of the second. Note that operations \emph{are} allowed to be moved from the
outside to the inside of the critical section. \code{acq\_rel} is the
\code{release} ordering when used with a store, and \code{acquire} when used
with a load. \code{seq\_cst} is similar to \code{acq\_rel}, but it also
guarantees that all threads see all sequentially consistent operations is the
same order.

We can use these ordering constraints to improve our example. Fences are
expensive, so we would rather want to just make sure that there is an ordering
relationship between \code{f} and \code{x}. We can obtain this by using
\code{Acquire} and \code{Release} semantics on \code{f}, like in
Listing~\ref{lst:acqrel}. The \code{Release} in thread B ensures that the
assignment \code{x = 1} is not moved after the store to \code{f}, and the
\code{Acquire} load in thread A ensures that the access to \code{x} is not moved
above the load of \code{f}. This gives us the desired semantics.

\begin{figure}[ht]
\begin{lstlisting}[caption=Synchronization using orderings,label=lst:acqrel]
// Thread A                      // Thread B
while f.load(Acquire) == 0 { }   x = 1
print(x)                         f.store(1, Release)
\end{lstlisting}
\end{figure}

\subsection{Concurrency in Rust}

The Rust standard library contains multiple primitives for concurrency.
\code{std::sync} contains types such as \code{Arc} (atomic reference counted
smart pointer), \code{Condvar} (condition variable), \code{Mutex}, and
\code{RwLock} (Read-Write lock). The \code{std::sync::atomic} module contains
atomic primitives (but only for \code{bool}, \code{isize}, \code{usize}, and
\code{ptr}), and the \code{fence} function, which is a memory fence. All atomic
operations including the memory fence take an \code{Ordering} as the last
argument, which is the same memory orderings as LLVM uses (\code{monotonic} is
renamed to \code{Ordering::Relaxed}).

As seen in Section~\ref{sec:rust} Rust forbids on shared mutability, but atomic
are made for shared mutability. For this reason, the functions on the atomic
types does not take \code{\&mut self}, but \code{\&self}, even though the
operation do mutate the value. This is an example of a case where we use
\code{\&mut} and \code{\&} not to signal mutability, but to signal whether it
is safe to perform the operation concurrently on multiple threads.

The \code{Mutex} is yet another example of a type that uses the RAII pattern.
\code{Mutex::new} takes the value that the mutex is protecting, and
\code{Mutex::lock} returns a new type, \code{MutexGuard}, which implements
\code{Deref<T>} and which unlocks the mutex in \code{Drop}. Wrapping the data
the mutex protects in the Mutex type makes it impossible to use the data without
acquiring the lock.


\section{The ABA Problem\label{sec:aba}}

The ABA problem is one of the most known problems in concurrent programming,
especially within the topic of memory reclamation. The essence of the problem is
that there are logical changes to a structure that we cannot observe due to
hardware limitations. For instance we might read a memory location twice,
observing no change in the memory read. We might then conclude that no change
has taken place. This conclusion however, might not be valid.

Consider the following real world analogy: assume you have a opaque bottle that
is filled with water. If you leave the bottle somewhere and return to it after
some time there is no way to see whether anyone has been drinking your water, by
simply inspecting the bottle from the outside. Someone might have taken the
bottle, drunk the water, and put the bottle back as it were. Even worse, someone
might have replaced your bottle with an identical bottle filled with bees.
Another example comes from in lock-free lists. Consider a linked list, where
nodes have a \code{next} pointer to the next element in the list.  By reading
the \code{next} field on a node twice at times $t\sb{0}$ and $t\sb{1}$, and
observing no change, we might conclude that the next node is the same. However,
the node at $t\sb{0}$ may have been removed from the list, its memory reclamed
for a new node, which is then inserted at the same place before $t\sb{1}$. This
can be disasterous and cause problems such as double deletes.

Instances of the ABA problem is easily prevented if we have atomic operations
that can check for change in the memory, even if it is changed back to the
initial state. An example of such operations are the \code{load-link} and
\code{store-conditional} instructions (\code{LL/SC}). The \code{SC} instruction
will only store a new value if the memory has not been written to after read by
\code{LL}. Current implementations on \code{LL/SC} are \emph{weak}, in the sense
that the store may fail even though it is never touched, but when another value
on the same cache line is written to. Architectures supporing \code{LL/SC}
include ARM, PowerPC, and RISC-V.

Another instruction that helps with the ABA problem is double compare-and-swap,
or \code{DCAS}, which is a two \code{CAS} operations that both must succeed for
any value to be updated. \code{DCAS} is not supported by any architecture. We
note that this is a different operation than double \emph{word} compare-and-swap
(\code{DWCAS}), which is a regular \code{CAS} that writes double word values.

Without propper hardware support there is no one good solution to the ABA
problem, but most instances of the problem are managable. Solution ideas include
\emph{tagging}, in which we tag the value swapped such that the second reading
observes a change in tag; \emph{indirection} in which we use an intermediate
node, such that the intermediate node changes, while the data node is still not
observed to have changed; and \emph{deferred reclamation} where we wait for
``safe periods'' in which we know that ABA problems cannot occur. In the
following chapter we look at reclamation schemes which utilizes all three of
these solution ideas.



\chapter{Memory Reclamation\label{ch:memory-reclamation}}

Dynamic memory allocation is a feature supported by the operating system. The
allocated memory must, sooner or later, be returned, or else there will
eventually be no more memory to allocate. The problem of concurrent memory
reclamation is all about finding out when it is safe to return memory to the
OS\@. The main source of problems in this field is that the threads may read
data, and then be preempted for an arbitrary long time; when it gets execution
time again, we must somehow make sure that the memory addresses it obtained are
either still valid, or that it can somehow validate them. Since threads can be
preempted at any point in execution, validation is a hard problem.

Most of the mainstream programming languages today has a large runtime which
employs a \emph{garbage collector}. The job of the garbage collector (GC) is to
find all memory that is no longer in use, and free it. A type of tracing garbage
colelctor is the \emph{mark-and-sweep GC}, which marks all pointers reachable
from some ground set of the program, sweeps the memory and frees the memory that
was not previously marked. This process involves pausing the program execution,
and is also dependent on the runtime thread.  One of the strengths of concurrent
systems is fail tolerance: thread may fail spuriously, but the system as a whole
makes progress regardless. It is clear that the runtime is a single point of
failure for systems employing one. It is therefore interesting to look at memory
management schemes that does not use a runtime with a garbage collector.
\todo{revisit this}

The field of concurrent memory reclamation is an active one, and a lot of
different schemes have emerged the recent years. We will look at \emph{Reference
Counting} (RC), \emph{Epoch Based Reclamation} (EBR), \emph{Hazard Pointers}
(HP), and try to give a highlight recent developments in the field.

\section{Reference Counting}
Reference counting (RC) is a natural solition for memory reclamation. It was
introduced in 1960 by G. E.  Collins\cite{collins1960method}, where it was used
for collecting nodes of a linked list.  The idea is that we count the number of
references to data, so that we can tell if we are holding the only reference to
some data. When we no longer need this reference, we know it is safe to reclaim
the memory the reference points to, since no other reference to that memory
exists. The primary downsides of RC is that it is rather expensive, and that a
na\"\i{}ve implementation does not reclaim cycles. Today reference counting is
still used, although it is unusual to have it be the primary mechanism for
memory management, due to its performance overhead.

Atoimc reference counting (ARC) is RC using atomic variables, and is a natural
extention of RC\@. However, the na\"\i{}ve implementation is not correct:
consider two threads operating on some \code{Rc<T>}. When thread $A$ want to
create a new reference to the data, it increments the count in the \code{RC}
object. Upon destruction, the count is decremented and the data is freed if the
count is 0. However, it is possible that thread $B$ has a reference to the RC
object and that it got preempted right before incrementing the count. Then the
whole object gets freed by thread $A$, since the count is 0, and when thread $B$
gets execution time again, it has a pointer to freed memory which it indents to
read.

A way to mitigate this problem is by indirection: we can use intermediate
\code{Rc} nodes which are the counter and a pointer to the actual data. The
intermediate nodes are never free'd, and by \code{CAS}ing the count to a
sentinel value upon destruction of the data, thread $B$ can detect that it is
about to read free'd memory and abort its operation. By allocation the \code{Rc}
objects with a memory arena and freeing them in bulk, this might be acceptable
for certain problems, as the data itself is not leaked, but only the \code{Rc}
nodes, which may be comparably small.

Despite the problem of atomic reference counting, there are still use cases for
it. A thread $A$ may create an \code{Arc} object, and make a copy of its
reference to it, incrementing the count, and only \emph{then} pass it to another
thread. This avoids the problem in the previous paragraphs, since the only
threads that need to increment the reference count is already holding onto
another reference, thus making it impossible that the count reaches zero before
we get to increment it. When all threads have dropped their reference, the
count will drop to zero, and the \code{Arc} will be free'd, not risking that any
other thread is just about to increment its count.
\clearpage
\section{Epoch Based Reclamation}

Epoch Based Reclamation (EBR) was introduced by Fraser
in~\cite{fraser2004practical}. It is a reclamation scheme based on the
observation that most programs have no references to internal data structure
memory in between of operations on the structure. The time interval in between
operations on the data structure are therefore safe-points (also called grace
periods) for memory reclamation to occur, since we do not risk invalidating any
data that other threads are using in this period. EBR uses the concept of an
\emph{epoch}, a global timestamp which we use to find out when it is safe to
reclaim retired memory. The epoch is a global counter. In addition we have a
global list with one entry for each running thread, which the threads use for
broadcasting their state, which includes the last epoch they read as well as
whether they are currently performing an operation. We call a thread performing
an operation \emph{pinned}, and the action of marking and unmarking
\emph{pinning} and \emph{unpinning} the thread.

When starting an operation a thread reads the global epoch, stores it in its
entry, and pins the thread. Upon retiring memory the thread marks the memory
with the global epoch and puts it in a \emph{limbo list}. Every once in a while,
the threads try to increment the epoch, which succeeds if all current pinned
threads have seen the current epoch. Note that we only have to look through the
thread entries once: if another thread is pinned while we are searching, it will
read the current epoch, and cause no problems for us. This requirement for
epoch incrementation means that all threads that have references to memory we
might want to free is either in the current or the previous epoch. Lastly, after
incrementing an epoch to $e$ we know that garbage that was added in epoch $e-2$
is safe to be freed.

Note that it is important that the thread inserting into the limbo list uses the
global epoch, and not the epoch it read when it was pinned. If we use the
previously read epoch, we may run into the following scenario:
\begin{enumerate}
  \item $A$ pins the thread at $e=5$, and wants to remove $O$ from the data structure.
  \item $B$ increments the epoch to $e=6$, and obtains a reference to $O$.
  \item $A$ unlinks $O$ from the data structure, and adds it to the limbo list,
    with $e=5$. $A$ unpinns, and increments the epoch to $e=7$.
  \item It is now safe, by our rules, to free $O$, although $B$ is still holding
    a reference to it.
\end{enumerate}
By reading the global epoch before pushing to the list we avoid this problem,
since $O$ is unlinked from the data structure before reading the epoch. This
makes it impossible for $B$ to have incremented the epoch, and \emph{then} get a
reference to $O$, without $A$ reading the incremented epoch.

EBR is very popular, due to its extremely low overhead.  However, there are
still a few challenges with EBR\@. A problem is that we are not allowed to keep
references to data across operations, since the thread must be pinned while we
are using the references. A natural way to mitigate this constraint is to leave
the thread pinned. However, this will stop the advancement of the global epoch,
and thus effectively halting the memory reclamation. An immediate consequence of
this is that EBR is not lock-free, which is not acceptable for all use cases.


\section{Hazard Pointers}

Hazard pointers were introduced by Michael in~\cite{michael2004hazard}.  The
paper formalizes hazardous pointers, and includes a proof of correctness. We
will settle for a informal view of them. It is based on the observation that in
most operations on data structure we only need a small constant number of
references to memory that is shared between running threads. The technique
exploits this by allowing each thread to register the pointers, called
\emph{hazard pointers}, the thread wants to use, but which it cannot be sure are
valid. We call potentially invalid pointers \emph{hazardous}. The number of
pointers we need varies with the algorithm performed, but a typical value is one
or two.

After reading a hazardous pointer the thread registers it as one of its hazard
pointers. It then have to \emph{validate} that the pointer is still in the data
structure, as it might have been removed in between the initial read and the
hazard registration. When we want to free memory we look through the hazard
pointers of all running threads. We note that again, as with EBR, a single pass
through this list is sufficient: the object is unlinked before searching, so if
a thread has a reference to it, but is yet to register it as one of its hazard
pointers, then it will fail validation.

If the memory is registered in a thread, we cannot immediately free the
memory. We now have two options: wait for the
thread to finish, or defer the deallocation.
By waiting on the thread, we are relying on that the other thread will ever
deregister the pointer. Hence, we give up lock-freedom, as this is prone to
deadlocking. It has, however, very low overhead, and will be very fast assuming
all threads are fairly scheduled and have similar work load.
Deferring the deallocation is a safer option, although it have a higher
overhead, eg.\ of pushing the pointer onto a queue. We would then occationally
visit the queue and see if any of the pointers in it have been deregistered by
all threads.

A challenge in usage of HP is that we need to identify which pointers in our
algorithms are hazardous. In comparison, we have no such concerns in EBR, in
which we only need to register memory as garbage when we remove it from the data
structure (we do need to make sure that this memory is only registered by a
single thread). Another challenge is that of validation, as there is no generic
way to do this. For most structures there is an obvious way of doing this. For
instance, in a queue we can validate the front element by reading the
\code{head} pointer again, observing that it has not changed. However, it stil
requires local knowledge of the data strcture in question.

\section{Other Schemes}
\todo{this. Debra, StackTrack, ++? We only need to write a paragraph on each or something.}


\chapter{Methodology\label{ch:methodology}}
\note{How did we implement the schemes? What choices did we have to take?
Whys. Also implementation problems, testing techniques, profiling setup etc.
Include std allocator change}


\section{Implementation}
There is often a disconnect between the idea of a system and the implementation
of that system. Algorithms are usually explained at such a high level that a
straight forward implementation in a given language is either impossible,
impractical, or leads to atypical code. Programming is more craft than
science, so when implementing a system we must reconsider the system in the
context of the language we are using.

The data structures were first implemented without any regard for memory
reclamation. Allocated memory were simply leaked when no longer needed.


\subsection{Atomics}

\note{Write about atomic types. If this is to be a section we need to clean up
the code in the atomics.rs files. We should probably also look into having HP as
a type in there, instead of doing the handle stuff. Then we can statically check
that hazardous accesses are behind a HP.}

As mentioned in Section~\ref{sec:rust}, it is idiomatic Rust to unitize the type
system in order to help ourselves. We have made other atomic types in addition
to those in the standard library in order to capture wanted semantics.


\subsection{Data Structures}
\label{sec:data-structures}

For comparing concurrent memory reclamation in a meaningful way we need shared
memory, organized in some data structure. We have chosen to implement two of the
most commonly seen concurrent data structures: the Queue and the List. Both
implementation are well known in the field of concurrent data structures. We
chose to implement these relative simple data structures due to time constraints
of the project.


\subsubsection{Queue}

The queue implemented is a Michael-Scott Queue, as described
in~\cite{michael1996simple}. \todo{Consider explaining how the MSqueue works}
The \code{Queue} and \code{Node} structs, as well as the signatures for the
public functions are listed in Listing~\ref{lst:msqueue}. The implementation of
\code{push} and \code{pop} is heavily inspired of the implementation from
Crossbeam\cite{crossbeam-msqueue}. We support \code{pop\_if}, since we use this
in the implementation of EBR. \code{push} allocates the memory needed for the
node, which in micro benchmarks have been shown to take the majority of the time:
allocation averaged at 54ns, while the rest of the procedure averaged at 30ns.
A natural optimization of this problem is to allocate nodes from a memory arena
or similar, such that the allocation overhead is amortized over multiple calls
to \code{push}. However, this seriously increases the complexity of the memory
management schemes.

\begin{figure}[ht]
\begin{lstlisting}[caption=Structs for the Michael-Scott
Queue,label=lst:msqueue,numbers=none]
pub struct Node<T> {
    data: ManuallyDrop<T>,
    next: Atomic<Node<T>>,
}
pub struct Queue<T> {
    head: Atomic<Node<T>>,
    tail: Atomic<Node<T>>,
}
impl<T> Queue<T> {
    pub fn new() -> Self;
    pub fn push(&self, T);
    pub fn pop(&self) -> Option<T>;
    pub fn pop_if(&self, Fn(&T) -> bool) -> Option<T>;
    pub fn is_empty(&self) -> bool;
}
\end{lstlisting}
\end{figure}

The \code{ManuallyDrop} type makes sure the data it wraps, in this case
\code{T}, is not automatically dropped when the node is dropped. We would rather
have the receiver of the data drop the values in the queue. \code{ManuallyDrop}
also makes it possible to \emph{do} drop the data. This is used in the
\code{Drop} implementation of the \code{Queue} itself, since we would like the
values in the queue to be cleaned up correctly when the queue is destroyed.

\subsubsection{List\label{sec:impl-list}}
\note{how did we implement the list?}

The list we have implemented is based on the list presented by Michael
in~\cite{michael2002high}. The implementation is similar to the Michael-Scott
Queue in multiple ways. For instance, we support a \code{remove\_front}
operation, which is almost identical to \code{Queue::pop}. However, the default
insertion operation of the List pushes the inserted element to the beginning of
the list, such that \code{List::insert} and \code{List::remove\_front} has LIFO
semantics. We also support removals for values which are comparable (implements
\code{PartialEq}). \note{assume we have \code{insert\_after} or something.} This
procedure complicates the implementation: consider the list in
Figure~\ref{fig:list-remove}. We want to remove node B, so we swing
\code{A.next} from B to C. However, at the same time, a thread might insert a
new node X in between B and C. When we now change \code{A.next} to \code{C}, we
have removed two items: B and X. This is an example of the ABA problem, which we
saw in Section~\ref{sec:aba}.

\begin{figure}[ht]
  \begin{subfigure}[b]{\textwidth}
      \centering
      \begin{tikzpicture}
        \node [lnode,label={A}] (A)              {\code{data} \nodepart{second} \code{}};
        \node [lnode,label={B}] (B) [right of=A] {\code{data} \nodepart{second} \code{}};
        \node [lnode,label={C}] (C) [right of=B] {\code{data} \nodepart{second} \code{}};
        \draw[-latex] ($ (A.west) - (0.5,0) $) -- (A.west);
        \draw[ptr] ($ (A.east) - (0.25,0) $) -- (B.west);
        \draw[ptr] ($ (B.east) - (0.25,0) $) -- (C.west);
        \draw[ptr] ($ (C.east) - (0.25,0) $) -- ($ (C.east) + (0.5, 0) $);
        \draw[-latex,color=lightgray] ($ (A.north) + (0.5,0) $) to[out=45,in=135] ($ (C.north) -
          (0.5, 0) $);
      \end{tikzpicture}
      \caption{The initial list. When removing B we swing the \code{}
      pointer over to C.\label{fig:list-remove-a}}
  \end{subfigure}

  \begin{subfigure}[b]{\textwidth}
      \centering
      \begin{tikzpicture}
        \node [lnode,label={A}] (A)              {\code{data} \nodepart{second} \code{}};
        \node [lnode,label={B}] (B) [right of=A] {\code{data} \nodepart{second} \code{}};
        \node [lnode,label={X}] (X) [right of=B] {\code{data} \nodepart{second} \code{}};
        \node [lnode,label={C}] (C) [right of=X] {\code{data} \nodepart{second} \code{}};
        \draw[-latex] ($ (A.west) - (0.5,0) $) -- (A.west);
        \draw[ptr] ($ (A.east) - (0.25,0) $) -- (B.west);
        \draw[ptr] ($ (B.east) - (0.25,0) $) -- (X.west);
        \draw[ptr] ($ (X.east) - (0.25,0) $) -- (C.west);
        \draw[ptr] ($ (C.east) - (0.25,0) $) -- ($ (C.east) + (0.5, 0) $);
        \draw[-latex,color=lightgray] ($ (A.north) + (0.5,0) $) to[out=45,in=135] ($ (C.north) -
          (0.5, 0) $);
      \end{tikzpicture}
      \caption{Another thread inserts a new node X between B and C.\label{fig:list-remove-b}}
  \end{subfigure}

  \begin{subfigure}[b]{\textwidth}
      \centering
      \begin{tikzpicture}
        \node [lnode,label={A}] (A)              {\code{data} \nodepart{second} \code{}};
        \node [lnode,label={\color{lightgray}B},color=lightgray] (B) [right of=A]
          {\code{data} \nodepart{second} \code{}};
        \node [lnode,label={\color{lightgray}X},color=lightgray] (X) [right of=B]
          {\code{data} \nodepart{second} \code{}};
        \node [lnode,label={C}] (C) [right of=X] {\code{data} \nodepart{second} \code{}};
        \draw[-latex] ($ (A.west) - (0.5,0) $) -- (A.west);
        \draw[ptr-g,color=lightgray] ($ (B.east) - (0.25,0) $) -- (X.west);
        \draw[ptr-g,color=lightgray] ($ (X.east) - (0.25,0) $) -- (C.west);
        \draw[ptr] ($ (C.east) - (0.25,0) $) -- ($ (C.east) + (0.5, 0) $);
        \draw[ptr] ($ (A.north) + (0.37,-0.26) $) to[out=45,in=135] ($ (C.north) -
          (0.5, 0) $);
      \end{tikzpicture}
      \caption{End result. B and X is no longer reachable from the head of the
        list.\label{fig:list-remove-c}}
  \end{subfigure}
  \caption{List removal which removes two elements when there is a concurrent
  insertion. This happens because the \code{compare-and-swap} operation
  performed on \code{A.next} is succeeds since the previous value is not
  changed.  However, the new value is logically changed, since C is no longer
  the \code{next} value of B, although there is no way for the \code{CAS} to
  detect that.\label{fig:list-remove}} \end{figure}

Our solution to this problem is simple and effective: we \emph{tag} the node we
want to remove, such that other threads do not add a new node after it. We
exploit the fact that memory addresses are aligned, meaning that the address of
an object in memory is divisible by some number. Objects are typically aligned
by their size, such that \code{u32}s are only on addresses where the four least
significant bits are zero. Bytes are ``aligned'' on 1 byte boundaries,
effectively meaning they are not aligned at all. Since we know that the pointers
we have points to larger objects, the LSB of the pointer itself is free. We use
this to tag the node.
\todo{double check which node we are tagging}
\note{Write about how insert checks for tag. What about searches? Note on search
- if the node is about to be removed, what do we report?}


\subsection{Epoch Based Reclamation}

Our implementation of EBR has both global state and thread local state. The data
in the global state contains three things: the epoch, which is an
\code{AtomicUsize}; a queue of retired garbage, which are tuples of pointers to
memory that we want to free and the epoch when the memory was retired; and a
list of thread markers, \code{List<ThreadMarker>}. This is all stored in the
\code{GlobalState} struct. We use the third-party crate \code{lazy\_static}
which contains a macro for lazy-initialized static variables. The thread local
data is stored in the \code{LocalState} struct, and is initialized using the
\code{thread\_local!} macro, which is in the Rust standard library. The thread
local data is a pointer to the threads list entry, a counter of how many times
the thread has been pinned, and buffered garbage.

Memory we want to free is hidden behind the \code{Garbage} struct, which
abstracts away the logic of calling the right destructor when we free the
memory. \todo{rewrite this part}. This is needed since
\code{LocalState::add\_garbage} takes an \code{Owned<T>}, an owned pointer to
any data type. The type information is then lost for the rest of the garbage
pipeline. However, upon destruction we need to know the type of the data we are
dropping. The way we have solved this is by moving the data to a closure, so
that the closure keeps track of the type of the data. The closure is never
invoked, but upon dropping the closure all of the values moved to it are
dropped. A advantage of this solution is that it is simple to implement. A
disadvantage is that the closure needs to be heap allocated, which increases the
overhead of adding garbage.  \todo{implement tags or something, and profile?}

\begin{figure}[ht!]
  \begin{tikzpicture}[ntitle/.style={node distance=1cm, font=\footnotesize}]
    \node (global-title) [anchor=west] {\normalsize\textbf{Global State}};

    \node [ntitle] (epoch-title) [below=of global-title.west, anchor=west] {Epoch:
      \code{AtomicUsize}};
    \node [draw=black!30, fit={(epoch-title)}]
      (epoch-box) {};

    \node [ntitle] (pins-title) [below=of epoch-title.west, anchor=west]
      {Thread Pin List: \code{List<ThreadPinMarker>}};
    {%
      \node [lnode,node distance=0.7cm]  (pins-f-a)  [below=of pins-title.west,
        anchor=west] {\code{marker} \nodepart{second} };
        \node [lnode,node distance=0.7cm]  (pins-f-b)  [right=of pins-f-a]
          {\code{marker} \nodepart{second} };
        \node [lnode,node distance=0.7cm]  (pins-f-c)  [right=of pins-f-b]
          {\code{marker} \nodepart{second} };
      \draw[ptr] ($ (pins-f-a.east) - (0.25,0) $) -- (pins-f-b.west);
      \draw[ptr] ($ (pins-f-b.east) - (0.25,0) $) -- (pins-f-c.west);
    }
    \node [draw=black!30, fit={(pins-title) (pins-f-c)}] (pins-box) {};

    \node [ntitle] (garbage-title) [below=of pins-f-a.west, anchor=west]
      {Garbage Queue: \code{Queue<(usize, Bag)>}};
    {%
      \node [lnode,node distance=.7cm]  (garbage-f-a)  [below=of garbage-title.west,
        anchor=west] {\code{(10, bag)} \nodepart{second} };
        \node [lnode,node distance=0.7cm]  (garbage-f-b)  [right=of garbage-f-a]
          {\code{(10, bag)} \nodepart{second} };
        \node [lnode,node distance=0.55cm]  (garbage-f-c)  [below=of garbage-f-a]
          {\code{(11, bag)} \nodepart{second} };
        \node [lnode,node distance=0.7cm]  (garbage-f-d)  [right=of garbage-f-c]
          {\code{(12, bag)} \nodepart{second} };
      \draw[ptr] ($ (garbage-f-a.east) - (0.25,0) $) -- (garbage-f-b.west);
      \draw[ptr] ($ (garbage-f-b.east) - (0.20, -0.05) $) --
        ($ (garbage-f-b.east) - (0.20,0.5) $) --
        ($ (garbage-f-c.north) - (0,-0.30) $) --
        (garbage-f-c.north);
      \draw[ptr] ($ (garbage-f-c.east) - (0.25,0) $) -- (garbage-f-d.west);
    }
    \node [draw=black!30, fit={(garbage-title) (garbage-f-d)}] (garbage-box) {};

    \node [draw=black, dotted, fit={(global-title) (pins-box) (garbage-box)}]
      (global-box) {};


    \node (local-title) [anchor=west, right=of global-title, shift={(3.5, 0)}]
      {\normalsize\textbf{Thread Local State}};

    \node [ntitle] (pinptr-title) [below=of local-title.west, anchor=west]
    {Thread Pin: \code{*const ThreadNodePtr}};
    \node [draw=black!30, fit={(pinptr-title) }]
      (pinptr-box) {};

    \node [ntitle] (pincount-title) [below=of pinptr-title.west, anchor=west]
    {Pin Count: \code{usize}};
    \node [draw=black!30, fit={(pincount-title) }]
      (pincount-box) {};

    \node [ntitle] (bag-title) [below=of pincount-title.west, anchor=west]
    {Garbage Bag: \code{Bag}};
    {%
      \node [lnode,node distance=.7cm,
      rectangle split parts=5
      ]  (bag-fig)  [below=of bag-title.west,
        anchor=west] {\nodepart{one}
          \code{garbage} \nodepart{two}
          \code{garbage} \nodepart{three}
          \code{garbage} \nodepart{four}
          \code{empty} \nodepart{five}
          \code{empty}
        };
    }
    \node [draw=black!30, fit={(bag-title) (bag-fig)}]
      (bag-box) {};

      \node [anchor=west, draw=black, dotted, fit={(local-title) (pinptr-box)
      (pincount-box) (bag-box)}]
      (local-box) {};
  \end{tikzpicture}
  \caption{The global and thread local data for the implemented EBR scheme. The
    global state contains the current Epoch, a list of thread pin markers, and a
    global garbage queue, in which garbage bags with an epoch is deferred for
    reclamation.  The thread pin in the thread local state points to that
    threads entry in the thread pin list. ``Pin Count'' is incremented each time
    the thread calls \code{pin}, and we use this number to choose when to
    reclaim memory from the global garbage queue. The thread local garbage bag
    is buffered garbage, so that we amortize the garbage queue overhead, since
    the nodes in \emph{that} list also have to be deferred for reclamation,
    using itself.\label{fig:ebr-impl}}
\end{figure}

We note that both the thread entry list and garbage queue themselves must be garbage
collected, and that we use EBR on them. This poses a problem: for each
garbage in the list, we need a node. However, that node itself will be garbage
when it is popped from the list, so we need to push it into the garbage list,
which makes a node, etc. Our solution to this is to chunk up garbage in chunks
of a constant size, using the \code{Bag} struct. Thus the garbage queue is a
\code{ebr::queue::Queue<(usize, Bag)>}. The chunking is done thread locally, which also
lowers synchronization overhead, since fewer elements are shared between threads.

When using the collections backed by EBR, we must first obtain a \code{Pin},
which is a proof that the current thread is pinned. All methods on any
collection requires a pin as the last argument, even though the methods may not
actually use the pin in the method body. There is no way to obtain the pin
directly: users must call the \code{pin} function, and pass in a closure which
is then given the pin as an argument. Listing~\ref{lst:pin-ex} shows example
usage on a queue. This design decision is made in order to discourage users to
grab a pin and keep it for a long amount of time, as this will effectively stop
the memory reclamation. Having a closure also makes it very clear when the
thread stops being pinned. Internally in the \code{ebr} module, there is a
method called \code{Pin::fake}, which makes a pin without actually pinning the
thread. This is an optimization used when we know that we have exclusive access
on certain memory.

\begin{figure}[ht!]
\begin{lstlisting}[caption=Example usage of the \code{pin} fucntion,
label=lst:pin-ex,numbers=none]
let queue = Queue::new();
pin(|pin| {
  queue.push(42, pin);
});
\end{lstlisting}
\end{figure}

The \code{pin} function is also where we reclaim the memory. Before calling the
closure passed in, we read the global epoch, and increment our local pin
counter. Every $n$th call to \code{pin}, we try to increment the epoch. For
incrementation to occur, all the pinned threads must have read the current
epoch.  If we succeed at incrementing the epoch we also free as much garbage as
allowed from the global garbage list. This scheme is in a sense fair, in that
threads that pin a lot supposedly create a lot of garbage, and these threads
will also have to clean up once in a while. It does however suffer from the fact
that a single thread is set to clean up all the garbage from other threads.
\todo{As I'm writing this, I realize that this is a terrible idea. Should fix.}

The \code{Pin} struct contains a method for retiring garbage, aptly called
\code{add\_garbage}. This method handles the thread local caching of garbage
objects into a \code{Bag}, and pushes the bag with the current epoch into the
global garbage queue when the bag is full. It does not support flushing the
current \code{Bag} (that is, adding it to the global queue even when it is not
full), although this is trivial to implement. \todo{What happens to garbage
when a thread dies? Leak?} Typical usage of \code{add\_garbage} is to make a
node unreachable by its data structure, read the data needed from it, and then
call \code{add\_garbage} with an \code{Owned<Node<T>>}.  \code{pin} and
\code{Pin::add\_garbage} are the only two methods a user of EBR needs to use.

The thread entry list is a potential memory problem:
each thread makes an entry the first time \code{pin} is called, and this entry
is never removed. Thus, if the system keeps creating threads which crashes right
after calling \code{pin}, the list will quickly grow, and its elements will
never be removed. This problem can be mitigated by removing the entry from the
list if the thread shuts down gracefully; however this is still a problem if
threads crashes. One solution could be to mark the entries with a timestamp, and
remove entries that have been inactive for too long, as well as to set the
threads thread local pointer to \code{null}. This creates further complications,
since thread may have simply been preempted for a long time, such that other
threads think it has crashed.  This problem has not been attempted solved as it
is mainly theoretical.\label{sec:thread-cleanup}

\subsection{Hazard Pointers}

The scheme for hazard pointers is simpler than that of EBR.\@ Each thread have a
\code{ThreadEntry} which contains a fixed size array of hazard pointers, which
are stored as \code{AtomicUsize}. The number of hazard pointers is stored in a
constant. The entries is stored in a global list, and each thread has a local
pointer to its entry in the list. The \code{Ptr} struct is extended with a
\code{hazard} method, which makes a \code{HazardPtr} struct.  \code{HazardPtr}
wraps an address which, and contains methods for registering and deregistering
the pointer in the threads list, checking if the pointer is registered by other
threads, and a spin-loop for waiting until all other threads have deregistered
the pointer.

Logically, a \code{HazardPtr} is a proof that the pointer is safe to use by the
thread. There is no abstraction around the validation of the hazard pointers;
this has to be done manually. Deregistering the hazard pointer is done in its
destructor.

\note{So far, HPs are freed after spinning. We should probably make a global
queue of retired HPs, and look through them every once in a while. Could use
\code{hazard()} to hijack execution?}





\section{Verification}
Programming concurrent systems is hard. Verifying \emph{any} system is also
hard. Therefore, we are lead to believe that verifying a concurrent system is
\emph{very} hard. This seems to be the case \note{this is probably too dumb
and ``funny''}.

While developing, we used unit tests. See Section~\ref{sec:rust-test} for a
primer on writing tests in Rust.

Testing were primarily done on the development machines of the author, which are
all \code{x86} machines. \todo{add note on bugs on ARM, if any}

The majority of the so-far found bugs in the system is memory bugs. Use after
free bugs, double free bugs, illegal memory accesses, and memory leaks. Double
frees and illegal memory accesses are usually hard faults, such that the
programs execution stops, and we are notified that eg. \code{0x8} is indeed not
a valid memory address to read from.

Use after free bugs and memory leaks are more difficult to find. Here we found
great use in Valgrind\cite{valgrind}, a instrumentation framework, which
contains the tool \code{memcheck}. \code{memcheck} intercepts all memory
operations, tracks allocations and frees, and is able to report most, if not
all, memory problems. One minor problem with using Valgrind with Rust is that
Valgrind has incomplete support of the default allocator in Rust,
\code{jemalloc}\cite{jemalloc}. However, changing the allocator in Rust to the system
allocator, which Valgrind supports fully, is possible.



\section{Profiling}
\note{How do we benchmark?}
\todo{actually find out this!}

We compare the performance overhead of HP, EBR, and without any reclamation. We
look at their performance implication when we operate on the queue and the list,
as described in Section~\ref{sec:data-structures}. We start out by looking at
the performance on a single threaded system, in order to get a notion of how how
much overhead the schemes impose. Then we look at a larger multi threaded system,
so see how well the schemes handle contention. \note{This sounds like a better
fit for the Results chapter?}

\note{Maybe we want to compare before and after comparisons after we have
optimized the code a little? Look at ordering implications?}

\todo{fix ``support testing''} While Rust do support testing, it does not
support benchmarking on the stable release. There exist multiple third party
benchmark suites, most of which are based on the nightly benchmark system. As
the implemented memory reclamation schemes uses global state and thread local
data, we must be careful in how the benchmark suite operates, due to the
problems noted in Section~\ref{sec:thread-cleanup}.  For these reasons, a
separate benchmark crate was developed. The system is simple and captures only
timings of which it reports the average and the variance. These timings were
used to generate the graphs in Chapter~\ref{ch:results}.

We set up multiple benchmarks, in order to try to find different scenarios in
which the schemes performs differently. We have \todo{} different benchmarks:
\begin{description}
  \item[\code{Queue::push}:] We measure the total time it takes to push \code{N}
    elements into a queue.
  \item[\code{Queue::pop}:] We measure the total time it takes to pop \code{N}
    elements out from a queue, which contains \code{N} elements. The threads
    repeatedly calls \code{pop} until \code{None} is returned.
  \item[\code{Queue::transport}:] We have two queues, one source and one sink. The
    source is pre-filled with \code{N} elements. The threads moves all elements
    from the source to the sink.
  \item[\code{List::remove}:] We pre-populate a list with \code{N} numbers in
    random order. Then each thread removes \code{N/t} unique elements, where
    \code{t} is the number of threads running. This ensures that all removals
    are succeding and that the list is empty at the end.
  \item[\code{List::real}:] We simulate a real world scenario, in which
    we perform different operations on the list. Which operation is performed is
    based on the following distribution: 40\% \code{insert}, 20\% \code{remove},
    20\% \code{remove\_front}, and 20\% \code{search}. The list is pre populated
    with some elements, in order for the removals not to fail often early on.
    The numbers which we insert, remove, or search for are also randomly
    generated.
\end{description}

An interesting problem that turned up when benchmarking is the following: how do
we benchmark the data structures that does not reclaim any memory? The problem
is that the benchmarking system runs a loop an unspecified number of times, to
make sure the results are statistically significant\note{this is probably not
the right word}. This means that we cannot directly control the number of nodes
we allocate which we do not free, and we risk running out of memory. This causes
the system to swap memory to disk, which destroys performance.

Fortunately, \code{bencher} does support executing code in between $n$ benchmark
iterations, where we get to specify $n$. We can use this to preallocate a
\code{Vec}, in which we put pointers to the memory we allocate in the data
structure we profile. Then, after the $n$ iterations, we free that memory. This
requires some \code{unsafe} code, but it is fully supported by the \code{Vec}
from the standard library. This is a better technique than to allocate the nodes
up front and pass the \code{Node} to the procedures, as this would hide the
allocation cost of the procedure. Listing~\ref{lst:bench-nothing} shows a
benchmark of a Queue using this technique.

\subsection{Single threaded Benchmarks}

\begin{figure}[ht]
  \begin{lstlisting}[caption=Microbenchmark of a data structure without memory
  reclamation,label=lst:bench-nothing]
pub fn push(b: &mut Bencher) {
    const N: u64 = 1024 * 1024;
    b.bench_n(N, |b| {
        let queue = Queue::new();
        let mut ptrs = Vec::with_capacity(N as usize);
        let ptr = ptrs.as_mut_ptr();
        let mut i = 0;
        b.iter(|| {
            queue.push(0usize, unsafe { ptr.offset(i) });
            i += 1;
        });
        unsafe {
            ptrs.set_len(N as usize);
        }
    });
}
  \end{lstlisting}
\end{figure}


\subsection{Multithreaded Benchmarks}


\note{Should add note on microbenchmarks. \code{cargo bench} is alright, but we
get very varying results, since we amortize cleanup over a many runs. This makes
the results given not so good.}


\chapter{Results}\label{ch:results}

We have ran the benchmark suite on five different machines, in order to cover
CPUs from different vendors made for different use cases, hoping to uncover
differences in the reclamation scheme performance. As the CPUs have a very
different core and thread count, the number of threads in each benchmark is not
the same across CPUs, but are capped with the number of hardware threads
supported for the CPU\@. This means that the benchmarks for the lower end CPUs
only use up to 4 threads, while the ones for the highly parallel ARM CPUs use
up to 32. The CPUs we have tested on are a Intel\textregistered{} i7--7500U @
2.70GHz with 2 cores and 4 threads, a Intel\textregistered{} i7--4770 @ 3.40GHz
with 4 cores and 8 threads, a Intel\textregistered{} Xeon\textregistered{}
E5--2620 @ 2.0GHz with 6 cores and 12 threads, a Ryzen 7 1700 @ 3.0GHz with 8
cores and 16 threads, and the ARM based Cavium ThunderX @ 2.5GHz with 32 cores
and 32 threads.

The benchmakrs are grouped by CPU, each of which have their own section.  Within
each section each benchmark have a single bar graph, showing the difference in
runtime from the version without any garbage collection. The y-axis is
logarithmic, and all shown timings are positive (any negative data points have
been set to 0).

We note that one of the schemes we compare, Crossbeam, is a well developed open
source library, while the remaining, namely EBR, HP, and HP-Spin, are the
authors own implementation. It is therefore expected that Crossbeams offers
lower overhead of its operations. Crossbeam has no implemented concurrent list,
so it is only present in the queue benchmarks.  In addition, the queue
implementations differ between Crossbeam and the author's. For this reason there
are some anomalies in the data, like Crossbeam being faster than no memory
reclamation. It is worth nothing that memory reclamation have been shown to
increase performance in \cite{brown2015reclaiming}, due to the reduced memory
footprint and hence a reduction in cache misses.

\newcommand{\benches}[2]{
  \begin{figure}[ht]
    \centering
    \begin{subfigure}{0.40\textwidth}
      \centering \footnotesize\code{Queue::Pop}
      \includegraphics[width=\textwidth]{plots/#1-b:queue_pop.pdf}
    \end{subfigure}
    \begin{subfigure}{0.40\textwidth}
      \centering \footnotesize\code{Queue::Transfer}
      \includegraphics[width=\textwidth]{plots/#1-b:queue_transfer.pdf}
    \end{subfigure}
    \\\vspace{0.25cm}
    \begin{subfigure}{0.40\textwidth}
      \centering \footnotesize\code{List::Remove}
      \includegraphics[width=\textwidth]{plots/#1-b:list_remove.pdf}
    \end{subfigure}
    \begin{subfigure}{0.40\textwidth}
      \centering \footnotesize\code{List::Real}
      \includegraphics[width=\textwidth]{plots/#1-b:list_real.pdf}
    \end{subfigure}
    \caption{Benchmarks relative to no memory reclamation for the #2}
  \end{figure}
}

\clearpage
\section{Intel\textregistered{} i7--7500U}

The i7--7500U is a high end CPU for the mobile segment. It has four hardware
threads and is hence the CPU with the fewest threads in the report.

\benches{laptop}{Intel\textregistered{} i7--7500U}

\clearpage
\section{Intel\textregistered{} i7--4770}
\benches{gribb}{Intel\textregistered{} i7--4770}

\clearpage
\section{Intel\textregistered{} Xeon\textregistered{} E5--2620}
\benches{server}{Intel\textregistered{} Xeon\textregistered{} E5--2620}

\clearpage
\section{Ryzen 7 1700}
\benches{ryzen}{Ryzen 7 1700}

\clearpage
\section{Cavium ThunderX}
\benches{scaleway}{Cavium ThunderX}


\chapter{Discussion\label{ch:discussion}}
\note{Whys. How was Rust? Why are the graphs as they are?
What alternatives exists?}


\bibliographystyle{acm}
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{sources}

\begin{appendices}
  \chapter{Rust's Toolchain}
  The Rust toolchain consists of multiple tools. There is the compiler
  \rustc{}, the build tool and dependency manager \cargo{}, and the toolchain
  manager \rustup.  \rustup{} is the preferred way of installing
  Rust\todo{source rustup.rs}.  \rustup{} also handles cross-compiling, i.e.\
  compiling programs for a different architecture.

  A major part of Rust's ecosystem is \url{https://crates.io}, the package repository
  for Rust. This is where packages handled by \cargo{} is downloaded from, by default.

  \section{Hello World}
  We show how to install the Rust toolchain on a Unix based system.
  Installing \rustup{} is done by downloading and running an install script from
  \url{https://rustup.rs}:
  \begin{lstlisting}[language=Bash,numbers=none]
% curl https://sh.rustup.rs -sSf | sh
  \end{lstlisting}
  This installs \rustup{}, \cargo{}, and \rustc{}.
  Next we want to use \cargo{} to make a project. This is done by \code{cargo init}.
  \begin{lstlisting}[language=Bash,numbers=none]
% cargo init --bin <name-of-project>
  \end{lstlisting}
  This will create a directory of the name provided, containing two files:
  \code{Cargo.toml} and \code{src/main.rs}.
  The former is the configuration file of the project, which contains:
  metadata such as the project name, version, authors;
  build options such as optimization levels, debug levels, optional flags;
  and dependencies, with versioning and optional flags.
  The initial \code{Cargo.toml} may look like Listing~\ref{lst:cargo.toml}.
  The other file, \code{src/main.rs} contains the entry point of the program, as
  in Listing~\ref{lst:hello-world}.
  \clearpage{}

  \begin{figure}[ht]
  \begin{lstlisting}[language=,label=lst:cargo.toml,
  caption=A newly generated \code{Cargo.toml}]
[package]
name = "project-name"
version = "0.1.0"
authors = ["Martin Hafskjold Thoresen <martinhath@gmail.com>"]

[dependencies]
  \end{lstlisting}
\end{figure}

\begin{figure}[ht]
  \begin{lstlisting}[caption=Hello World in Rust,label=lst:hello-world]
fn main() {
    println!("Hello, world!");
}
  \end{lstlisting}
\end{figure}

  To run the program, we use \cargo{}:

  \begin{figure}[ht]
  \begin{lstlisting}[language=Bash,numbers=none]
% cargo run
   Compiling project-name v0.1.0 (file:///<path>/)
    Finished dev [unoptimized + debuginfo] target(s) in 0.68 secs
     Running `target/debug/project-name`
Hello, world!
%
  \end{lstlisting}
\end{figure}

  Cargo supports a project to build multiple executables, or no executables at all.
  For more information about cargo see~\url{http://doc.crates.io/index.html}.

  \section{\code{\#[test]} and \code{\#[bench]}}
  \label{sec:rust-test}
  The Rust toolchain supports both testing and benchmarking. To write tests we
  make an inline module named \code{test}, and conditionally compile it using
  \code{\#[cfg(test)]}. This makes the code to be ignored unless we are running
  the tests; this is done with \code{cargo test}. Listing~\ref{lst:cargo-test}
  shows an example test on a \code{Queue}. This is usually put in the same file
  at the \code{Queue} but this is only by convention, and not required.

  \begin{figure}[ht]
  \begin{lstlisting}[label=lst:cargo-test,caption=An example test in Rust]
#[cfg(test)]
mod test {
    use super::*;
    #[test]
    fn queue() {
        let mut queue = Queue::new();
        queue.push(1);
        queue.push(2);
        assert_eq!(queue.pop(), Some(1));
        assert_eq!(queue.pop(), Some(2));
    }
}
    \end{lstlisting}
  \end{figure}
  The output of \code{cargo test} might look like
  Listing~\ref{lst:cargo-test-output}.
  \begin{figure}[ht]
    \begin{lstlisting}[label=lst:cargo-test-output,caption=Sample output of
    \code{cargo test},language=,numbers=none]
running 47 tests
test ebr::atomic::tests::valid_tag_i8 ... ok
test ebr::atomic::tests::valid_tag_i64 ... ok
test ebr::bench::pin ... ok
test ebr::queue::bench::push ... ok
test ebr::queue::test::can_construct_queue ... ok
test ebr::queue::test::is_unique_receiver ... ok
    \end{lstlisting}
  \end{figure}

  \todo{Replace this with using \code{bencher}.}
  Benchmarking is similar, except that we do not have a conditional compilation
  flag for benchmarks. For similarity, we can put benchmarks in the \code{bench}
  module. Benchmarks are annotated with \code{\#[bench]}. The benchmarks are ran
  with \code{cargo bench}, which runs all benchmark annotated functions.
  Listing~\ref{lst:bench-test} shows a sample benchmark, and
  Listing~\ref{lst:bench-test-output} shows sample output from a benchmark.
  Note that the benchmark system is not in stable Rust, so we need to use the
  nightly version. The code that is benchmarked is the closure passed to
  \code{test::Bencher::iter}.

  \begin{figure}[ht]
  \begin{lstlisting}[label=lst:bench-test,caption=An example benchmark in Rust]
mod bench {
    extern crate test;
    use super::Queue;
    #[bench]
    fn push(b: &mut test::Bencher) {
        let q = Queue::new();
        b.iter(|| {
            ::ebr::pin(|pin| {
                q.push(1, pin);
            });
        });
    }
}
    \end{lstlisting}
  \end{figure}
  \begin{figure}[ht]
    \begin{lstlisting}[label=lst:bench-test-output,caption=Sample output of
    \code{cargo bench},language=,numbers=none]
test ebr::bench::pin               ... bench:   18 ns/iter (+/- 0)
test ebr::queue::bench::push       ... bench:   57 ns/iter (+/- 12)
test hp::list::bench::remove_front ... bench:    2 ns/iter (+/- 0)
    \end{lstlisting}
  \end{figure}


  \clearpage,
  \section{Setup for cross-compilation}
  Other targets may be installed through \rustup{}. For instance, if we want to
  make \code{aarch64\-unknown-linux-gnu} available for cross-compilation we would run

  \begin{lstlisting}[language=Bash,numbers=none]
% rustup target add aarch64-unknown-linux-gnu
  \end{lstlisting}

  We can either pass in the target architecture to \cargo{} at each invocation,
  or we can configure \cargo{} to use another target by default. We show the latter.
  We make a new file in the project directory called \code{.cargo/config}
  containing Listing~\ref{lst:cargo/config}

  \begin{figure}[ht]
  \begin{lstlisting}[language=,
                     label=lst:cargo/config,
                     caption=Cargo configuration file for cross-compiling]
[build]
target = "aarch64-unknown-linux-gnu"

[target.aarch64-unknown-linux-gnu]
linker = "aarch64-linux-gnu-gcc"
  \end{lstlisting}
  \end{figure}
  This sets the default target to be \code{aarch-unknown-linux-gnu},
  and specifies the linked to be used.
  After this \code{cargo build} builds for the specified target.
  The executable can be found in \code{./target/aarch-unknown-linux-gnu/debug/}
  with the name of the project.


  \chapter{Benchmark Results}

\newcommand{\figuregrid}[1]{

  \rotatebox{90}{\hspace{-0.6cm}\footnotesize{\code{Queue::push}}}
  \foreach \scheme in {nothing, crossbeam, ebr, hp, hpspin} {%
    \begin{subfigure}{0.20\textwidth}
      \centering \footnotesize{\code{\scheme}}
      \includegraphics[width=\textwidth]{plots/#1-s:\scheme-b:queue_push}
    \end{subfigure}}

    \rotatebox{90}{\hspace{-0.6cm}\footnotesize{\code{Queue::pop}}}
  \foreach \scheme in {nothing, crossbeam, ebr, hp, hpspin} {%
    \begin{subfigure}{0.20\textwidth}
      \includegraphics[width=\textwidth]{plots/#1-s:\scheme-b:queue_pop}
    \end{subfigure}}

    \rotatebox{90}{\hspace{-0.9cm}\footnotesize{\code{Queue::transfer}}}
  \foreach \scheme in {nothing, crossbeam, ebr, hp, hpspin} {%
    \begin{subfigure}{0.20\textwidth}
      \includegraphics[width=\textwidth]{plots/#1-s:\scheme-b:queue_transfer}
    \end{subfigure}}

  \rotatebox{90}{\hspace{-0.6cm}\footnotesize{\code{List::remove}}}
    \begin{subfigure}{0.20\textwidth}
      \includegraphics[width=\textwidth]{plots/gribb-s:nothing-b:list_remove}
    \end{subfigure}
  \makebox[0.20\textwidth]{}
  \foreach \scheme in {ebr, hp, hpspin} {%
    \begin{subfigure}{0.20\textwidth}
      \includegraphics[width=\textwidth]{plots/gribb-s:\scheme-b:list_remove}
    \end{subfigure}}

    \rotatebox{90}{\hspace{-0.6cm}\footnotesize{\code{List::Real}}}
    \begin{subfigure}{0.20\textwidth}
      \includegraphics[width=\textwidth]{plots/gribb-s:nothing-b:list_real}
    \end{subfigure}
  \makebox[0.20\textwidth]{}
  \foreach \scheme in {ebr, hp, hpspin} {%
    \begin{subfigure}{0.20\textwidth}
      \includegraphics[width=\textwidth]{plots/gribb-s:\scheme-b:list_real}
    \end{subfigure}}
}

\todo{Write something about these benchmarks, and what might be interesting to look at here.}

\clearpage
\section{Intel\textregistered{} i7--7500U}
\begin{figure}[ht]
  \figuregrid{laptop}
\end{figure}

\clearpage
\section{Intel\textregistered{} i7--4770}
\begin{figure}[ht]
  \figuregrid{gribb}
\end{figure}

\clearpage
\section{Intel\textregistered{} Xeon\textregistered{} E5--2620}
\begin{figure}[ht]
  \figuregrid{server}
\end{figure}

\clearpage
\section{Ryzen 7 1700}
\begin{figure}[ht]
  \figuregrid{ryzen}
\end{figure}

\clearpage
\section{Cavium ThunderX}
\begin{figure}[ht]
  \figuregrid{scaleway}
\end{figure}



\end{appendices}

\end{document}
