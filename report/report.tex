\documentclass[b5paper]{report}
\usepackage[lmargin=25mm,rmargin=25mm,tmargin=27mm,bmargin=30mm]{geometry}

\usepackage[toc]{appendix}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{color}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{parskip}
\usetikzlibrary{positioning,shapes,decorations,calc,fit,arrows.meta}

\usepackage{libertine}
\usepackage{inconsolata}
\usepackage{libertinust1math}
\usepackage[T1]{fontenc}
% \definecolor{font}{RGB}{40, 40, 40}
% TODO: remove
% \color{font}

\begin{document}
\title{Concurrent Memory Reclamation on ARM}
\author{Martin Hafskjold Thoresen}
\date{\today}
\maketitle

\input{macros.tex}

\begin{abstract}
  \todo{update this!}
  Concurrent programs, like any other program, produces garbage In languages
  that do not have a garbage collector, freeing used memory needs to be
  handled by the programmer, or through other means by the language This
  report compares 3 different memory reclamation schemes: epoch based
  reclamation, hazard pointers, and reference counting We look at how they
  perform when used in different common concurrent data structures, such as
  Queues, Lists, and (probably not) Skip-Lists, on the ARM platform.
\end{abstract}

\tableofcontents

\chapter{Introduction}
\note{Outline: parallelism, garbage collection, what we are about to do,
rust, performance, usability}

\chapter{Background}

\note{I meant to write a quick intro on parallelism, and why we care. Maybe this
this is out of scope?}
In 1965 Gordon Moore stated what is now knows as \emph{Moore's Law}, which
roughly states that the number of transistors on integrated circuits will
double every 18 months. The law has also been used to describe the clock speed
of central processing units (CPU's). However, this exponential growth came to a
halt during the 2000s, where the clock speed of CPUs stopped between 3 GHz and
4 GHz Over 10 years later high end consumer CPUs still have a clock speed in
that same range, and the performance increase of modern CPUs has mainly been
due to increase in parallelism\note{has it, though? Or is it more that CPUs
hasn't really improved that much?}. This improvement however, has not been as
noticeable as the former improvements in clock speed. This is because programs
already written cannot automatically use the extra parallelism that newer
architectures give us; Parallelism needs to be accounted for by programmers.

\note{fix this. It reads alright on its own, but the previous paragraph ends
with the same thing, making it very weird to read.}
Despite having multi-core architectures as the most central performance
improvement in CPUs, parallelism is not a success story for programmers. Most
mainstream programming languages does not have first class support for parallel
constructs, and synchronization and work management\note{?} must still be
handled explicitly by the programmer, using fairly low level constructs, such
as mutexes and threads \note{what are alternatives?}.
\note{what is the motivation behind this? Do we want to make the claim that
Rust is making low level stuff more approachable, and that we can help
improving the performance of programs written by having CMR in Rust?}
In short, parallelism has yet to be approachable for a lot of programmers.
\note{If we are going to complain on the state of parallel programming we
should at least mention some alternatives.}
\note{Parallelism isn't a success story for programmers. Hard, error-prone,
even simple things goes wrong. Need effective abstractions to improve this.??}


\subsection*{\note{TODO LIST}}
\note{What do we need to understand in order to appreciate this report?
Moore's law, parallel applications, multi core, blabla\\
Something about ARM.\\
maybe notes on state of the art concurrency stuff?
\\
Notes on memory terminology (retire, reclaim, etc.)
\\
Consider adding MS queue or something to show pitfalls in concurrent programming?
}

\section{Terminology}
We start with basic terminology of memory management. Most of the programs we
write \emph{allocate} memory, meaning requesting a memory range from the OS for
exclusive use by the program. This memory is then \emph{deallocated}, or
\emph{freed}, when we are done with it, so that the memory may be used at a
later time; the memory is thus \emph{reclaimed}. If we want to ``mark'' memory
as freed without freeing it (that is, the program still has exclusive use of
it) we say we \emph{retire} the memory (reasons for doing so will become
apparent). In concurrent settings, we risk accessing memory at the same time; a
\emph{data race}, or a \emph{race condition} is when multiple threads are
operating on the same memory at simultaneously, and where at least one operation
is a write.

Both modern compilers and CPUs may reorder program instructions if they think
it will improve performance, for instance by improving memory locality and thus
improving cache behavior, or reduce pipeline stalling. This may or may not be
acceptable by the program. The guarantee that the instructions executed appear
to take effect in program order is called \emph{sequential consistency}, and
was defined by Leslie Lamport in 1979\todo{source here}. We call a system in
which threads can be prevented from making progress \emph{blocking}. The
opposite of blocking is \emph{non-blocking}. A method is \emph{lock-free} if we
have a guarantee that \emph{some} thread will make progress. Note that any given
thread may be blocked for an infinite amount of time. The guarantee that any
given thread will eventually make progress is called \emph{wait-freedom}.  For a
more thorough introduction see~\cite{herlihy2011art}.


\note{parallel vs concurrent?}


\section{Rust}\label{sec:rust}

Rust~\cite{rust} is a programming language which focus is safety, performance,
and concurrency. It is is freely developed by over 1900 contributors on the
version control platform GitHub~\cite{github}, and is officially sponsored by
Mozilla. Rust 1.0 was released in May 2015, and the current stable version of
the language is 1.21, with a new version released every 6th week. The language
is compiled and typed, and includes features such as type inference, pattern
matching, tagged enums, template-like generics, and a minimal runtime without a
garbage collector. As of November 2017 the official Rust compiler \code{rustc}
uses LLVM~\cite{llvm} for optimization and code generation. The language has no
formal specification, and language changes are done through an RFC (``request
for comments'') process. The syntax of Rust is out of scope for this report, so
we will settle with a highlight of the most important features of Rust. For a
thorough introduction to Rust, see The Rust Programming Language\cite{trpl}.

As C++, Rust has structs. It does not however have classes or inheritance. The
type system is based around \emph{traits}, which are comparable to interfaces in
Java. A trait defines methods, optionally with an implementation. We can then
\emph{implement} the trait for a struct. Implementing a trait for a struct
requires that either the trait of the struct is defined in the \emph{crate} the
implementation is in. A consequence of this is that it is possible to extend
types from the standard library. It is possible to have values which type is a
trait, called \emph{trait objects}. This is useful when we want to be agnostic
of the implementation of an interface. However, this requires dynamic dispatch
of the function calls of the value.

Despite the lack of a garbage collector, Rust programmers does not have to
manually manage memory. Rust solves this through \emph{ownership} semantics:
values are either \emph{owned} or \emph{borrowed} by its scope. Transferring
ownership of a value is possible by \emph{moving} it. When a borrowed value goes
out of scope, nothing happens as a borrowed value is simply a pointer. When an
owned value goes out of scope it is \emph{dropped}, meaning its destructor is
ran. Rust's ownership rules, which are enforced by the compiler, guarantees that
there are no other references to the object when it is dropped. Implementing the
destructor of a type is done by implementing the \code{Drop} trait. There is no
trait for constructors, but by convention a method called \code{new} is used.

An example of an abstraction using both \code{new} and \code{Drop} is
\code{std::box::Box<T>}, a fat pointer to a heap allocated value of type
\code{T}. The \code{new} implementation allocates the memory, and \code{drop}
will free the memory. This pattern is often called \emph{RAII} (resource
allocation is initialization). This way we get a safe abstraction over heap
allocation, since the memory is freed exactly once when the \code{Box} goes out
of scope.  \code{Box<T>} also implements another trait, \code{Deref<T>}, which
causes \code{Box} references to automatically be converted by the compiler to
\code{T} references, when needed. We note that we can \emph{not} pass a
\code{Box<T>} to a function that takes a \code{T}, since they have different
\code{drop} implementations.

In addition to the ownership rules, Rust have two important rules for
references: references are always valid, and aliased references where more than
one is mutable is not allowed. At compile time Rust tracks the \emph{lifetime}
of all values, and ensures that the lifetime of all references does not outlive
the lifetime of the data they are referencing. If a function returns a reference
to a value allocated on the stack of that function it is caught at compile time,
since the references lifetime would outlive the functions lifetime, and the
values lifetime is at most that of the function. The second rule stops one
reference to destroy the data that another reference points to. For instance it
is not allowed (and caught at compile time) to have a reference to a value
inside a \code{Vec} and mutate the \code{Vec}, since mutating the \code{Vec}
might cause the data to be moved, which invalidates the reference.

\subsection{Unsafe Rust}

Sometimes the rules of Rust are too strict. Rust, aiming to be a pragmatic
language, offers an escape hatch for this: \emph{unsafe code}. In unsafe code it
is allowed to: dereference raw pointers, call \code{unsafe} functions, implement
\code{unsafe} traits, and mutate static variables. Raw pointers are, as in C and
C++, just a memory address, and as with references, both a immutable
(\code{*const T}) and mutable (\code{*mut T}) variants exist. In unsafe Rust,
we enter the dangerous and error-prone territory that C and C++ programmers know
all too well. Examples of \code{unsafe} functions are all FFI functions,
unchecked array indexing, and creating a \code{String} without UTF-8 checks (all
\code{String}s in Rust are valid UTF-8).

\code{unsafe} code is often more about signaling to the programmer
that the code is fragile, and that one should put extra care into making sure
that ones assumptions are correct. Many of the \code{unsafe} functions in
\code{std} are versions of safe functions without error checking, which is
desirable when we know (or think we know) that we do not need them.

By exposing \code{unsafe} to programmers, one might argue that Rust is just as
unsafe as C or C++. The primary advantage of \code{unsafe} is that the ratio of
unsafe to safe code is very low. For instance, the Rust compiler itself consists
only of $1\%$ unsafe code\cite{rustc-unsafe}. This makes it possible to focus on
the few places in a code base that is unsafe when developing, testing, and code
reviewing. It is however important to realize that the effects of unsafe code is
non-local: bugs in unsafe code can cause undefined behavior to take effect in
perfectly safe code.



\section{Concurrency in Hardware}

Most programmers rarely have to think about the hardware their programs run on.
Caches, for instance, is designed to be transparent for programmers. With
concurrency this changes, as concurrent programming have challenges in which a
mental model of how the hardware works is paramount. In this section we consider
multi-core CPU's with multiple levels of cache; this includes most modern CPU's,
both desktop computers and laptops, as well as most mobile devices, such as
smartphones and tablets.

The cache hierarchy poses a problem for shared memory programs. The first cache
level, L1, is usually only used by one core, while the remaining levels, L2 (and
maybe L3), are shared. This means that when a program is writing to memory, the
same logical memory may be present in multiple physical locations on the CPU.\@
This causes synchronization problems when this memory is written to. This is the
problem of \emph{cache coherence}. A memory range does not even have to be used
by multiple threads: since caches operates in memory chunks, called \emph{cache
lines}, it is enough that two values are on the same cache line for
synchronization to occur. This is called \emph{false sharing}.

We would like our programs to be sequentially consistent, as this maps the
program execution to the source code of the program. However, we also want our
programs to be performant, and it turns out that from a hardware perspective
these two requirements are conflicting. CPUs are simply not sequentially
consistent.

CPU architectures have rules on how much the CPU is allowed to reorder reads and
writes without breaking the semantics of the program when ran sequentially. They
also have instructions explicitly for avoiding instruction reordering, called
\emph{memory fences} (or \emph{barriers}; we will use the former name). The x86
architecture have a rather strong \emph{memory model}, meaning the CPU is very
limited in its ability to reorder.  For instance, x86 forbids reads to be
reordered with other reads, and writes with other writes. It also forbids any
reordering with locked instructions; this is very strict, as the preferred
instruction used for sequential consistent atomic store is a locked instruction,
meaning atomic stores are full fences. ARM on the other hand, have a comparably
weak memory model. A disadvantage of this is that sequentially consistent
atomics require full fences for both loads and stores, since they are allowed to
be reordered. ARMv8 offers a solution to this by having explicit instructions
for SC loads and stores. For more information on x86, see Chapter 8.2.2
in~\cite{intel64}; for information about ARMv7 see~\cite{armv7-reference-manual},
and for ARMv8 see~\cite{armv8-reference-manual}.

Instruction sets have \emph{atomic} operations. An operation is atomic if its is
performed either fully or not at all. For instance, if thread A non-atomically
stores a value at a location, thread B may observe the write when it is only
partially performed. This is not possible using atomic instructions.  Atomic
load and store is the two most fundamental atomic operations, but most platforms
also give us additional instructions; the most notable begin
\code{compare-and-swap} (\code{CAS}). \code{CAS} takes three arguments: a memory
address, a value A, and a value B. If the value at the given address is A, it
writes B to the address. If not, nothing is done. \code{CAS} is a central
building block in concurrent programming, and especially in lock-free
programming.

\note{We can mention transactional memory, and stuff like that?}

\section{Concurrency in Software}

\note{Hardware works such and such, but how does programmers handle these
things?  Mention \code{volatile}? Look at instructions?  Many programmers does
not think about cache problems. What about concurrency problems? May they also
be ignored? (no)}

\note{Threads, Processes, CAS, FA, Mutex, volatile, }

We start the section with a motivating example of instruction reordering.
Consider Listing~\ref{lst:reordering}, and assume variables are initialized to
0.  When ran sequentially (first B and then A), we would observe no difference
if the assignments of thread B were reordered; the end result would be that we
print \code{1}. However, when run concurrently thread B might decide to assign
\code{f = 1} first, and thread A will risk printing \code{0} instead of
\code{1}. This outcome was impossible in the sequential world.
\begin{figure}[ht]
\begin{lstlisting}[caption=Instruction reordering,label=lst:reordering]
// Thread A                      // Thread B
while f == 0 { }                 x = 1
print(x)                         f = 1
\end{lstlisting}
\end{figure}

A solution to this problem is to use a memory fence, in order to explicitly
disallow any reordering past it, as in Listing~\ref{lst:mem-fence}.
\begin{figure}[ht]
\begin{lstlisting}[caption=Memory fence for synchronization,label=lst:mem-fence]
// Thread A                      // Thread B
while f == 0 { }                 x = 1
fence()                          fence()
print(x)                         f = 1
\end{lstlisting}
\end{figure}

LLVM defines a memory model which is inspired by the C++11 memory
model~\cite{llvmmm}. The model includes six \emph{memory ordering constraints},
which are used in atomic operations. The orderings dictates how the compiler and
the CPU is allowed to reorder the instructions around operations the orderings
are used on.  We look at five of them here: \code{monotonic}, \code{acquire},
\code{release}, \code{acq\_rel}, and \code{seq\_cst}.  \code{monotonic} is the
weakest and offers no ordering constraints. Atomic operations with this ordering
differs only from regular operations in that the operation cannot be observed to
happen partially.  \code{acquire} and \code{release} are intended to work in
pairs, by loading with \code{acquire} and storing with \code{release} in the
same memory location.  An \code{acquire} load of X ensures that subsequent loads
will see all stores that happend before a \code{release} store to X.  This can
be used to implement a lock, where we \code{acquire} the lock before the
critical section, and \code{release} the lock after it.  Now we get the
guarantee that two critical sections cannot overlap, since the stores before the
\code{realease} in the first section must be visible upon the \code{acquire}
load of the second. Note that operations \emph{are} allowed to be moved from the
outside to the inside of the critical section. \code{acq\_rel} is the
\code{release} ordering when used with a store, and \code{acquire} when used
with a load. \code{seq\_cst} is similar to \code{acq\_rel}, but it also
guarantees that all threads see all sequentially consistent operations is the
same order.

We can use these ordering constraints to improve our example. Fences are
expensive, so we would rather want to just make sure that there is an ordering
relationship between \code{f} and \code{x}. We can obtain this by using
\code{Acquire} and \code{Release} semantics on \code{f}, like in
Listing~\ref{lst:acqrel}. The \code{Release} in thread B ensures that the
assignment \code{x = 1} is not moved after the store to \code{f}, and the
\code{Acquire} load in thread A ensures that the access to \code{x} is not moved
above the load of \code{f}. This gives us the desired semantics.

\begin{figure}[ht]
\begin{lstlisting}[caption=Synchronization using orderings,label=lst:acqrel]
// Thread A                      // Thread B
while f.load(Acquire) == 0 { }   x = 1
print(x)                         f.store(1, Release)
\end{lstlisting}
\end{figure}

\subsection{Concurrency in Rust}

The Rust standard library contains multiple primitives for concurrency.
\code{std::sync} contains types such as \code{Arc} (atomic reference counted
smart pointer), \code{Condvar} (condition variable), \code{Mutex}, and
\code{RwLock} (Read-Write lock). The \code{std::sync::atomic} module contains
atomic primitives (but only for \code{bool}, \code{isize}, \code{usize}, and
\code{ptr}), and the \code{fence} function, which is a memory fence. All atomic
operations including the memory fence take an \code{Ordering} as the last
argument, which is the same memory orderings as LLVM uses (\code{monotonic} is
renamed to \code{Ordering::Relaxed}).

As seen in Section~\ref{sec:rust} Rust forbids on shared mutability, but atomic
are made for shared mutability. For this reason, the functions on the atomic
types does not take \code{\&mut self}, but \code{\&self}, even though the
operation do mutate the value. This is an example of a case where we use
\code{\&mut} and \code{\&} not to signal mutability, but to signal whether it
is safe to perform the operation concurrently on multiple threads.

The \code{Mutex} is yet another example of a type that uses the RAII pattern.
\code{Mutex::new} takes the value that the mutex is protecting, and
\code{Mutex::lock} returns a new type, \code{MutexGuard}, which implements
\code{Deref<T>} and which unlocks the mutex in \code{Drop}. Wrapping the data
the mutex protects in the Mutex type makes it impossible to use the data without
acquiring the lock.


\section{The ABA Problem\label{sec:aba}}

The ABA problem is one of the most known problems in concurrent programming,
especially within the topic of memory reclamation. The essence of the problem is
that there are logical changes to a structure that we cannot observe due to
hardware limitations. For instance we might read a memory location twice,
observing no change in the memory read. We might then conclude that no change
has taken place. This conclusion however, might not be valid.

Consider the following real world analogy: assume you have a opaque bottle that
is filled with water. If you leave the bottle somewhere and return to it after
some time there is no way to see whether anyone has been drinking your water, by
simply inspecting the bottle itself. Someone might have taken the bottle, drunk
the water, and put the bottle back as it were. Even worse, someone might have
replaced your bottle with an identical bottle filled with snakes.

An more relevant example is in lock-free lists. Consider a linked list, where
nodes have a \code{next} pointer to the next element in the list.  By reading
the \code{next} field on a node twice at times $t_0$ and $t_1$, we might
conclude that the next node is the same. However, the node at $t_0$ may have
been removed from the list, its memory reclamed for a new node, which is then
inserted at the same place before $t_1$. This can be disasterous if we read the
\code{next} field of the node at $t_0$, for instance if we are trying to remove
the node.

Instances of the ABA problem is easily prevented if we have atomic operations
that can check for change in the memory, even if it is changed back to the
initial state. An example of such operations are the \code{load-link} and
\code{store-conditional} instructions (\code{LL/SC}). The \code{SC} instruction
will only store a new value if the memory has not been written to after read by
\code{LL}. Current implementations on \code{LL/SC} are \emph{weak}, in the sense
that the store may fail even though it is never touched, but when another value
on the same cache line is written to. Architectures supporing \code{LL/SC}
include ARM, PowerPC, and RISC-V.

Another instruction that helps with the ABA problem is double compare-and-swap,
or \code{DCAS}, which is a two \code{CAS} operations that both must succeed for
any value to be updated. \code{DCAS} is not supported by any architecture. We
note that this is a different operation than double \emph{word} compare-and-swap
(\code{DWCAS}), which is a regular \code{CAS} that writes values that are double
words.

Without propper hardware support there is no one good solution to the ABA
problem, but most instances of the problem are managable. Solution ideas include
\emph{tagging}, in which we tag the value swapped such that the second reading
observes a change in tag; \emph{indirection} in which we use an intermediate
node, such that the intermediate node changes, while the data node is still not
observed to have changed; and \emph{deferred reclamation} where we wait for
``safe periods'' in which we know that ABA problems cannot occur. In the
following chapter we look at reclamation schemes which utilizes all three of
these solution ideas.



\chapter{Memory Reclamation}

\note{What is memory reclamation? Why is it hard?  What approaches are there?}
\note{Assumed background: had OS, have programmed in C or C++.}

% When programming we lend memory from the operating system. This memory must be
% returned, or else we will sooner or later run out of it. In most modern
% programming languages, this is a feature provided by the runtime of the
% language. We call such languages \emph{managed languages}. However, in
% languages such as Rust, there is virtually no runtime, so this becomes a
% concern of the programmer Garbage collection is typically the name used for
% memory reclamation; if we want to pedantic we would say a garbage collector
% performs memory reclamation.
%
% In a concurrent setting, we are concerned about the properties of our data
% structures, such as wait-freedom, or lock-freedom.
% \todo{rewrite this sentence}. One can make the claim that data structures
% implemented in managed languages can never obtain some of these properties,
% since they depend on the runtime to get execution time in order for the program
% to be able to allocate memory. Hence, the runtime thread has to get CPU time
% in order for the program to make progress \note{Could say this better; the
% point is to motivate the usage of Rust.}

The field of concurrent memory reclamation is an active one, and a lot of
different schemes have emerged the recent years. We will look at
\emph{Reference Counting} (RC),
\emph{Epoch Based Reclamation} (EBR),
\emph{Hazard Pointers} (HP),
\note{and maybe a few others?}




\section{Reference Counting}
\todo{this}
Simple, "obvious" solution. But does it work?

\todo{split into two paragraphs? Eventually move history part to bottom, Knuth
style}. The idea of reference counting is that we count the number of references
to data, so that we can tell if we are holding the only reference to some
data. When we no longer need this reference, we know it is safe to reclaim the
memory the reference points to, since no other reference to that memory
exists. Reference counting first appeared in 1960 by G. E.
Collins\cite{collins1960method}, where it was used for collecting nodes of a
linked list. The primary downsides of RC is that it is rather expensive, and
that a na\"\i{}ve implementation does not reclaim cycles. Today reference
counting is still used, although not in the setting of general memory
reclamation. \todo{source?} \note{Want to say that while RC isn't used in GC,
it is used for other things, like \code{std::rc::Rc} stuff. Also \code{Arc},
even more expensive}.

% TODO: Use this for node drawing, when needed.
%
% \begin{figure}[ht]
% \begin{tikzpicture}
% \tikzset{Node/.style={
%   rectangle split,
%   rectangle split horizontal,
%   rectangle split parts=2,
%   draw,
%   rounded corners=0.1cm
%   }}
%       \node [Node] (A) at (0,0) {\code{Count=3} \nodepart{second} \code{DATA} };\\
%       \node [Node] (B) at (4,0) {\code{Count} \nodepart{second} \code{DATA} };\\
%       \draw[->]  (0.5,-1) -- (0.5,-0.3);
%       \draw[->]  (0,-1) -- (0,-0.3);
%       \draw[->]  (-0.5,-1) -- (-0.5,-0.3);
% \end{tikzpicture}
%   \caption{Data nodes using RC}
% \end{figure}

\todo{Consider adding a code listing instead of using words} Reference counting
is a natural approach to the problem of concurrent memory reclamation. The
first observation to make is that we need to use atomic reads and writes to
correctly increment and decrement the reference count. However, the na\"\i{}ve
implementation is not correct. Consider two threads operating on some
\code{RC<T>}. When thread A want to create a new reference to the data, it
increments the count in the \code{RC} object. Upon destruction, the count is
decremented and the data is freed if the count is 0. However, it is possible
that thread B has a reference to the RC object and that it got preempted right
before incrementing the count. Then the whole object gets freed by thread A,
since the count is 0, and when thread B gets execution time again, it has a
pointer to freed memory which it indents to read. It is worth noting that we
can free the \emph{data} using this scheme, but not the entire \code{RC} node,
since the \code{count} field may be set to a special value indicating that its
data is freed.

\note{Add note on differential RC:\@
http://www.1024cores.net/home/lock-free-algorithms/object-life-time-management/differential-reference-counting}


% TODO:  Code listing sample here, with custom lineno prefix
%
% \begin{lstlisting}[label=lst:rc-broken,
%                    numberblanklines=false,
%                    % TODO: would like to not copy this all over the place
%                    numberstyle=\color{gray}\ttfamily{}RC]
% node = load_node();
% node.count += 1;
% data = node.data;
% // use the node
% node.count -=1;
% if node.count == 0
%   free(node);
% \end{lstlisting}

\section{Epoch Based Reclamation}
Epoch Based Reclamation (EBR) was introduced by Fraser
in~\cite{fraser2004practical}. It is a reclamation scheme based on the
observation that most programs have no references to data structure memory in
between of operations on the structure. The time interval in between
operations on the data structure are therefore safe-points (also called grace
periods) for memory reclamation to occur \todo{add something more here}

EBR uses the concept of an \emph{epoch}, a global timestamp which we use to
find out when it is safe to reclaim retired memory. The epoch is a global
counter. In addition we have a global list with one entry for each running
thread, which the thread uses for registering the last epoch they read, as well
as whether they are currently performing an operation. We call a thread
performing an operation \emph{pinned}, and the action of marking and unmarking
\emph{pinning} and \emph{unpinning} the thread.

When starting an operation a thread reads the global epoch and saves it in its
entry, and pins the thread. Upon retiring memory the thread marks the memory
with the global epoch and puts it in a \emph{limbo list} \note{replace with the
three lists instead of marking?}. Every once in a while, the threads try to
increment the epoch. The epoch can only be incremented if all pinned threads
have seen the current epoch. This way we know that all threads with references
to memory which may be freed by the data structure is either in the current
epoch, or in the previous epoch. Lastly, after incrementing an epoch to $e$ we
know that garbage that was added in epoch $e-2$ is safe to be freed.
\note{Should clarify here. Note that the potential counter example is A a wants
to delete X, B incs epoch and reads X, A deletes X, and incs epoch. But, now X
is marked with e+1, even though A was in e, since we're reading the global
epoch on delete.}

\note{Add example from comere?}

There are still a few challenges with EBR A problem is that we are not
allowed to keep references to data across operations, since the thread must be
pinned while we are using the references. A natural way to mitigate this
constraint is to leave the thread pinned. However, this will stop the
advancement of the global epoch, and thus effectively halting the memory
reclamation. An immediate consequence of this is that EBR is not lock-free.
\note{Limits/challenges of EBR. Add more.}

\todo{Add proof? Pseudocode of stuff?}


\section{Hazard Pointers}

Hazard pointers was introduced by Michael in~\cite{michael2004hazard}.
The paper by Michael formalizes hazardous pointers, and includes a proof of
correctness. We will settle for a informal view of them. It is
based on the observation that in most operations on data structure we only need
a small constant number of references to memory that is shared between running
threads. The technique exploits this by allowing each thread to register the
pointers, called \emph{hazard pointers}, the thread wants to use, but which it
cannot be sure are valid. We call such pointers \emph{hazardous}. The number of
pointers we need varies with the algorithm performed, but a typical value is one
or two.

As an example, the $next$ pointers in a linked list would be registered as
hazardous, as they might point to memory that is already removed from the list.
By registering the pointers that are hazardous, other threads can refrain from
changing the referenced data. As with RC there are some complications: the
thread might be preempted after reading the hazardous pointer, but before
registering it as hazardous; then another thread may have removed the referenced
data. For this reason, we must \emph{validate} the reference after registering
it as hazardous. This can be problematic for structures in which there is no
obvious way of verifying the data. For other structures this is just a matter of
reading a pointer again.

A challenge in usage of HP is that we need to identify which pointers in our
algorithms are hazardous. In comparison, we have no such concerns in EBR, in
which we only need to register memory as garbage when we remove it from the data
structure (we do need to make sure that this memory is only registered by a
single thread). This imposes the choice of memory reclamation onto the
programmer of the data structures using them. In other words, it is unlikely
that HP can be used ``transparently'', in the sense that we can seamlessly
change between, say, HP and EBR\@.

\todo{Revisit RC here, with the stuff from the blog?
Or maybe its better to just put it at the end of the subsection.}


\section{Other Schemes}
\note{We don't have time to implement all interesting schemes,
but it would be nice to have them in the report}


\chapter{Methodology}
\note{How did we implement the schemes? What choices did we have to take?
Whys. Also implementation problems, testing techniques, profiling setup etc.
Include std allocator change}


\section{Implementation}
There is often a disconnect between the idea of a system and the implementation
of that system. Algorithms are usually explained at such a high level that a
straight forward implementation in a given language is either impossible,
impractical, or leads to atypical code. Programming is more craft than
science, so when implementing a system we must reconsider the system in the
context of the language we are using.

The data structures were first implemented without any regard for memory
reclamation. Allocated memory were simply leaked when no longer needed.


\subsection{Atomics}

\note{Write about atomic types. If this is to be a section we need to clean up
the code in the atomics.rs files. We should probably also look into having HP as
a type in there, instead of doing the handle stuff. Then we can statically check
that hazardous accesses are behind a HP.}

As mentioned in Section~\ref{sec:rust}, it is idiomatic Rust to unitize the type
system in order to help ourselves. We have made other atomic types in addition
to those in the standard library in order to capture wanted semantics.


\subsection{Data Structures}
\label{sec:data-structures}

For comparing concurrent memory reclamation in a meaningful way we need shared
memory, organized in some data structure. We have chosen to implement two of the
most commonly seen concurrent data structures: the Queue and the List. Both
implementation are well known in the field of concurrent data structures. We
chose to implement these relative simple data structures due to time constraints
of the project.


\subsubsection{Queue}

The queue implemented is a Michael-Scott Queue, as described
in~\cite{michael1996simple}. \todo{Consider explaining how the MSqueue works}
The \code{Queue} and \code{Node} structs, as well as the signatures for the
public functions are listed in Listing~\ref{lst:msqueue}. The implementation of
\code{push} and \code{pop} is heavily inspired of the implementation from
Crossbeam\cite{crossbeam-msqueue}. We support \code{pop\_if}, since we use this
in the implementation of EBR. \code{push} allocates the memory needed for the
node, which in micro benchmarks have been shown to take the majority of the time:
allocation averaged at 54ns, while the rest of the procedure averaged at 30ns.
A natural optimization of this problem is to allocate nodes from a memory arena
or similar, such that the allocation overhead is amortized over multiple calls
to \code{push}. However, this seriously increases the complexity of the memory
management schemes.

\begin{figure}[ht]
\begin{lstlisting}[caption=Structs for the Michael-Scott
Queue,label=lst:msqueue,numbers=none]
pub struct Node<T> {
    data: ManuallyDrop<T>,
    next: Atomic<Node<T>>,
}
pub struct Queue<T> {
    head: Atomic<Node<T>>,
    tail: Atomic<Node<T>>,
}
impl<T> Queue<T> {
    pub fn new() -> Self;
    pub fn push(&self, T);
    pub fn pop(&self) -> Option<T>;
    pub fn pop_if(&self, Fn(&T) -> bool) -> Option<T>;
    pub fn is_empty(&self) -> bool;
}
\end{lstlisting}
\end{figure}

The \code{ManuallyDrop} type makes sure the data it wraps, in this case
\code{T}, is not automatically dropped when the node is dropped. We would rather
have the receiver of the data drop the values in the queue. \code{ManuallyDrop}
also makes it possible to \emph{do} drop the data. This is used in the
\code{Drop} implementation of the \code{Queue} itself, since we would like the
values in the queue to be cleaned up correctly when the queue is destroyed.

\subsubsection{List\label{sec:impl-list}}
\note{how did we implement the list?}

The list we have implemented is based on the list presented by Michael
in~\cite{michael2002high}. The implementation is similar to the Michael-Scott
Queue in multiple ways. For instance, we support a \code{remove\_front}
operation, which is almost identical to \code{Queue::pop}. However, the default
insertion operation of the List pushes the inserted element to the beginning of
the list, such that \code{List::insert} and \code{List::remove\_front} has LIFO
semantics. We also support removals for values which are comparable (implements
\code{PartialEq}). \note{assume we have \code{insert\_after} or something.} This
procedure complicates the implementation: consider the list in
Figure~\ref{fig:list-remove}. We want to remove node B, so we swing
\code{A.next} from B to C. However, at the same time, a thread might insert a
new node X in between B and C. When we now change \code{A.next} to \code{C}, we
have removed two items: B and X. This is an example of the ABA problem, which we
saw in Section~\ref{sec:aba}.

\begin{figure}[ht]
  \begin{subfigure}[b]{\textwidth}
      \centering
      \begin{tikzpicture}
        \node [lnode,label={A}] (A)              {\code{data} \nodepart{second} \code{}};
        \node [lnode,label={B}] (B) [right of=A] {\code{data} \nodepart{second} \code{}};
        \node [lnode,label={C}] (C) [right of=B] {\code{data} \nodepart{second} \code{}};
        \draw[-latex] ($ (A.west) - (0.5,0) $) -- (A.west);
        \draw[ptr] ($ (A.east) - (0.25,0) $) -- (B.west);
        \draw[ptr] ($ (B.east) - (0.25,0) $) -- (C.west);
        \draw[ptr] ($ (C.east) - (0.25,0) $) -- ($ (C.east) + (0.5, 0) $);
        \draw[-latex,color=lightgray] ($ (A.north) + (0.5,0) $) to[out=45,in=135] ($ (C.north) -
          (0.5, 0) $);
      \end{tikzpicture}
      \caption{The initial list. When removing B we swing the \code{}
      pointer over to C.\label{fig:list-remove-a}}
  \end{subfigure}

  \begin{subfigure}[b]{\textwidth}
      \centering
      \begin{tikzpicture}
        \node [lnode,label={A}] (A)              {\code{data} \nodepart{second} \code{}};
        \node [lnode,label={B}] (B) [right of=A] {\code{data} \nodepart{second} \code{}};
        \node [lnode,label={X}] (X) [right of=B] {\code{data} \nodepart{second} \code{}};
        \node [lnode,label={C}] (C) [right of=X] {\code{data} \nodepart{second} \code{}};
        \draw[-latex] ($ (A.west) - (0.5,0) $) -- (A.west);
        \draw[ptr] ($ (A.east) - (0.25,0) $) -- (B.west);
        \draw[ptr] ($ (B.east) - (0.25,0) $) -- (X.west);
        \draw[ptr] ($ (X.east) - (0.25,0) $) -- (C.west);
        \draw[ptr] ($ (C.east) - (0.25,0) $) -- ($ (C.east) + (0.5, 0) $);
        \draw[-latex,color=lightgray] ($ (A.north) + (0.5,0) $) to[out=45,in=135] ($ (C.north) -
          (0.5, 0) $);
      \end{tikzpicture}
      \caption{Another thread inserts a new node X between B and C.\label{fig:list-remove-b}}
  \end{subfigure}

  \begin{subfigure}[b]{\textwidth}
      \centering
      \begin{tikzpicture}
        \node [lnode,label={A}] (A)              {\code{data} \nodepart{second} \code{}};
        \node [lnode,label={\color{lightgray}B},color=lightgray] (B) [right of=A] {\code{data} \nodepart{second} \code{}};
        \node [lnode,label={\color{lightgray}X},color=lightgray] (X) [right of=B] {\code{data} \nodepart{second} \code{}};
        \node [lnode,label={C}] (C) [right of=X] {\code{data} \nodepart{second} \code{}};
        \draw[-latex] ($ (A.west) - (0.5,0) $) -- (A.west);
        \draw[ptr-g,color=lightgray] ($ (B.east) - (0.25,0) $) -- (X.west);
        \draw[ptr-g,color=lightgray] ($ (X.east) - (0.25,0) $) -- (C.west);
        \draw[ptr] ($ (C.east) - (0.25,0) $) -- ($ (C.east) + (0.5, 0) $);
        \draw[ptr] ($ (A.north) + (0.37,-0.26) $) to[out=45,in=135] ($ (C.north) -
          (0.5, 0) $);
      \end{tikzpicture}
      \caption{End result. B and X is no longer reachable from the head of the
        list.\label{fig:list-remove-b}}
  \end{subfigure}
  \caption{List removal which removes two elements when there is a concurrent
  insertion. This happens because the \code{compare-and-swap} operation
  performed on \code{A.next} is succeeds since the previous value is not
  changed.  However, the new value is logically changed, since C is no longer
  the \code{next} value of B, although there is no way for the \code{CAS} to
  detect that.\label{fig:list-remove}} \end{figure}

Our solution to this problem is simple and effective: we \emph{tag} the node we
want to remove, such that other threads do not add a new node after it. We
exploit the fact that memory addresses are aligned, meaning that the address of
an object in memory is divisible by some number. Objects are typically aligned
by their size, such that \code{u32}s are only on addresses where the four least
significant bits are zero. Bytes are ``aligned'' on 1 byte boundaries,
effectively meaning they are not aligned at all. Since we know that the pointers
we have points to larger objects, the LSB of the pointer itself is free. We use
this to tag the node.
\todo{double check which node we are tagging}
\note{Write about how insert checks for tag. What about searches? Note on search
- if the node is about to be removed, what do we report?}


\subsection{Epoch Based Reclamation}

Our implementation of EBR has both global state and thread local state. The data
in the global state contains three things: the epoch, which is an
\code{AtomicUsize}; a queue of retired garbage, which are tuples of pointers to
memory that we want to free and the epoch when the memory was retired; and a
list of thread markers, \code{List<ThreadMarker>}. This is all stored in the
\code{GlobalState} struct. We use the third-party crate \code{lazy\_static}
which contains a macro for lazy-initialized static variables. The thread local
data is stored in the \code{LocalState} struct, and is initialized using the
\code{thread\_local!} macro, which is in the Rust standard library. The thread
local data is a pointer to the threads list entry, a counter of how many times
the thread has been pinned, and buffered garbage.

Memory we want to free is hidden behind the \code{Garbage} struct, which
abstracts away the logic of calling the right destructor when we free the
memory. \todo{rewrite this part}. This is needed since
\code{LocalState::add\_garbage} takes an \code{Owned<T>}, an owned pointer to
any data type. The type information is then lost for the rest of the garbage
pipeline. However, upon destruction we need to know the type of the data we are
dropping. The way we have solved this is by moving the data to a closure, so
that the closure keeps track of the type of the data. The closure is never
invoked, but upon dropping the closure all of the values moved to it are
dropped. A advantage of this solution is that it is simple to implement. A
disadvantage is that the closure needs to be heap allocated, which increases the
overhead of adding garbage.  \todo{implement tags or something, and profile?}

\begin{figure}[ht!]
  \begin{tikzpicture}[ntitle/.style={node distance=1cm, font=\footnotesize}]
    \node (global-title) [anchor=west] {\normalsize\textbf{Global State}};

    \node [ntitle] (epoch-title) [below=of global-title.west, anchor=west] {Epoch:
      \code{AtomicUsize}};
    \node [draw=black!30, fit={(epoch-title)}]
      (epoch-box) {};

    \node [ntitle] (pins-title) [below=of epoch-title.west, anchor=west]
      {Thread Pin List: \code{List<ThreadPinMarker>}};
    {%
      \node [lnode,node distance=0.7cm]  (pins-f-a)  [below=of pins-title.west,
        anchor=west] {\code{marker} \nodepart{second} };
        \node [lnode,node distance=0.7cm]  (pins-f-b)  [right=of pins-f-a]
          {\code{marker} \nodepart{second} };
        \node [lnode,node distance=0.7cm]  (pins-f-c)  [right=of pins-f-b]
          {\code{marker} \nodepart{second} };
      \draw[ptr] ($ (pins-f-a.east) - (0.25,0) $) -- (pins-f-b.west);
      \draw[ptr] ($ (pins-f-b.east) - (0.25,0) $) -- (pins-f-c.west);
    }
    \node [draw=black!30, fit={(pins-title) (pins-f-c)}] (pins-box) {};

    \node [ntitle] (garbage-title) [below=of pins-f-a.west, anchor=west]
      {Garbage Queue: \code{Queue<(usize, Bag)>}};
    {%
      \node [lnode,node distance=.7cm]  (garbage-f-a)  [below=of garbage-title.west,
        anchor=west] {\code{(10, bag)} \nodepart{second} };
        \node [lnode,node distance=0.7cm]  (garbage-f-b)  [right=of garbage-f-a]
          {\code{(10, bag)} \nodepart{second} };
        \node [lnode,node distance=0.55cm]  (garbage-f-c)  [below=of garbage-f-a]
          {\code{(11, bag)} \nodepart{second} };
        \node [lnode,node distance=0.7cm]  (garbage-f-d)  [right=of garbage-f-c]
          {\code{(12, bag)} \nodepart{second} };
      \draw[ptr] ($ (garbage-f-a.east) - (0.25,0) $) -- (garbage-f-b.west);
      \draw[ptr] ($ (garbage-f-b.east) - (0.20, -0.05) $) --
        ($ (garbage-f-b.east) - (0.20,0.5) $) --
        ($ (garbage-f-c.north) - (0,-0.30) $) --
        (garbage-f-c.north);
      \draw[ptr] ($ (garbage-f-c.east) - (0.25,0) $) -- (garbage-f-d.west);
    }
    \node [draw=black!30, fit={(garbage-title) (garbage-f-d)}] (garbage-box) {};

    \node [draw=black, dotted, fit={(global-title) (pins-box) (garbage-box)}]
      (global-box) {};


    \node (local-title) [anchor=west, right=of global-title, shift={(3.5, 0)}]
      {\normalsize\textbf{Thread Local State}};

    \node [ntitle] (pinptr-title) [below=of local-title.west, anchor=west]
    {Thread Pin: \code{*const ThreadNodePtr}};
    \node [draw=black!30, fit={(pinptr-title) }]
      (pinptr-box) {};

    \node [ntitle] (pincount-title) [below=of pinptr-title.west, anchor=west]
    {Pin Count: \code{usize}};
    \node [draw=black!30, fit={(pincount-title) }]
      (pincount-box) {};

    \node [ntitle] (bag-title) [below=of pincount-title.west, anchor=west]
    {Garbage Bag: \code{Bag}};
    {%
      \node [lnode,node distance=.7cm,
      rectangle split parts=5
      ]  (bag-fig)  [below=of bag-title.west,
        anchor=west] {\nodepart{one}
          \code{garbage} \nodepart{two}
          \code{garbage} \nodepart{three}
          \code{garbage} \nodepart{four}
          \code{empty} \nodepart{five}
          \code{empty}
        };
    }
    \node [draw=black!30, fit={(bag-title) (bag-fig)}]
      (bag-box) {};

      \node [anchor=west, draw=black, dotted, fit={(local-title) (pinptr-box)
      (pincount-box) (bag-box)}]
      (local-box) {};
  \end{tikzpicture}
  \caption{The global and thread local data for the implemented EBR scheme. The
    global state contains the current Epoch, a list of thread pin markers, and a
    global garbage queue, in which garbage bags with an epoch is deferred for
    reclamation.  The thread pin in the thread local state points to that
    threads entry in the thread pin list. ``Pin Count'' is incremented each time
    the thread calls \code{pin}, and we use this number to choose when to
    reclaim memory from the global garbage queue. The thread local garbage bag
    is buffered garbage, so that we amortize the garbage queue overhead, since
    the nodes in \emph{that} list also have to be deferred for reclamation,
    using itself.\label{fig:ebr-impl}}
\end{figure}

We note that both the thread entry list and garbage queue themselves must be garbage
collected, and that we use EBR on them. This poses a problem: for each
garbage in the list, we need a node. However, that node itself will be garbage
when it is popped from the list, so we need to push it into the garbage list,
which makes a node, etc. Our solution to this is to chunk up garbage in chunks
of a constant size, using the \code{Bag} struct. Thus the garbage queue is a
\code{ebr::queue::Queue<(usize, Bag)>}. The chunking is done thread locally, which also
lowers synchronization overhead, since fewer elements are shared between threads.

When using the collections backed by EBR, we must first obtain a \code{Pin},
which is a proof that the current thread is pinned. All methods on any
collection requires a pin as the last argument, even though the methods may not
actually use the pin in the method body. There is no way to obtain the pin
directly: users must call the \code{pin} function, and pass in a closure which
is then given the pin as an argument. Listing~\ref{lst:pin-ex} shows example
usage on a queue. This design decision is made in order to discourage users to
grab a pin and keep it for a long amount of time, as this will effectively stop
the memory reclamation. Having a closure also makes it very clear when the
thread stops being pinned. Internally in the \code{ebr} module, there is a
method called \code{Pin::fake}, which makes a pin without actually pinning the
thread. This is an optimization used when we know that we have exclusive access
on certain memory.

\begin{figure}[ht!]
\begin{lstlisting}[caption=Example usage of the \code{pin} fucntion,
label=lst:pin-ex,numbers=none]
let queue = Queue::new();
pin(|pin| {
  queue.push(42, pin);
});
\end{lstlisting}
\end{figure}

The \code{pin} function is also where we reclaim the memory. Before calling the
closure passed in, we read the global epoch, and increment our local pin
counter. Every $n$th call to \code{pin}, we try to increment the epoch. For
incrementation to occur, all the pinned threads must have read the current
epoch.  If we succeed at incrementing the epoch we also free as much garbage as
allowed from the global garbage list. This scheme is in a sense fair, in that
threads that pin a lot supposedly create a lot of garbage, and these threads
will also have to clean up once in a while. It does however suffer from the fact
that a single thread is set to clean up all the garbage from other threads.
\todo{As I'm writing this, I realize that this is a terrible idea. Should fix.}

The \code{Pin} struct contains a method for retiring garbage, aptly called
\code{add\_garbage}. This method handles the thread local caching of garbage
objects into a \code{Bag}, and pushes the bag with the current epoch into the
global garbage queue when the bag is full. It does not support flushing the
current \code{Bag} (that is, adding it to the global queue even when it is not
full), although this is trivial to implement. \todo{What happens to garbage
when a thread dies? Leak?} Typical usage of \code{add\_garbage} is to make a
node unreachable by its data structure, read the data needed from it, and then
call \code{add\_garbage} with an \code{Owned<Node<T>>}.  \code{pin} and
\code{Pin::add\_garbage} are the only two methods a user of EBR needs to use.

The thread entry list is a potential memory problem:
each thread makes an entry the first time \code{pin} is called, and this entry
is never removed. Thus, if the system keeps creating threads which crashes right
after calling \code{pin}, the list will quickly grow, and its elements will
never be removed. This problem can be mitigated by removing the entry from the
list if the thread shuts down gracefully; however this is still a problem if
threads crashes. One solution could be to mark the entries with a timestamp, and
remove entries that have been inactive for too long, as well as to set the
threads thread local pointer to \code{null}. This creates further complications,
since thread may have simply been preempted for a long time, such that other
threads think it has crashed.  This problem has not been attempted solved as it
is mainly theoretical.\label{sec:thread-cleanup}

\subsection{Hazard Pointers}

The scheme for hazard pointers is simpler than that of EBR.\@ Each thread have a
\code{ThreadEntry} which contains a fixed size array of hazard pointers, which
are stored as \code{AtomicUsize}. The number of hazard pointers is stored in a
constant. The entries is stored in a global list, and each thread has a local
pointer to its entry in the list. The \code{Ptr} struct is extended with a
\code{hazard} method, which makes a \code{HazardPtr} struct.  \code{HazardPtr}
wraps an address which, and contains methods for registering and deregistering
the pointer in the threads list, checking if the pointer is registered by other
threads, and a spin-loop for waiting until all other threads have deregistered
the pointer.

Logically, a \code{HazardPtr} is a proof that the pointer is safe to use by the
thread. There is no abstraction around the validation of the hazard pointers;
this has to be done manually. Deregistering the hazard pointer is done in its
destructor.

\note{So far, HPs are freed after spinning. We should probably make a global
queue of retired HPs, and look through them every once in a while. Could use
\code{hazard()} to hijack execution?}





\section{Verification}
Programming concurrent systems is hard. Verifying \emph{any} system is also
hard. Therefore, we are lead to believe that verifying a concurrent system is
\emph{very} hard. This seems to be the case \note{this is probably too dumb
and ``funny''}.

While developing, we used unit tests. See Section~\ref{sec:rust-test} for a
primer on writing tests in Rust.

Testing were primarily done on the development machines of the author, which are
all \code{x86} machines. \todo{add note on bugs on ARM, if any}

The majority of the so-far found bugs in the system is memory bugs. Use after
free bugs, double free bugs, illegal memory accesses, and memory leaks. Double
frees and illegal memory accesses are usually hard faults, such that the
programs execution stops, and we are notified that eg. \code{0x8} is indeed not
a valid memory address to read from.

Use after free bugs and memory leaks are more difficult to find. Here we found
great use in Valgrind\cite{valgrind}, a instrumentation framework, which
contains the tool \code{memcheck}. \code{memcheck} intercepts all memory
operations, tracks allocations and frees, and is able to report most, if not
all, memory problems. One minor problem with using Valgrind with Rust is that
Valgrind has incomplete support of the default allocator in Rust,
\code{jemalloc}\cite{jemalloc}. However, changing the allocator in Rust to the system
allocator, which Valgrind supports fully, is possible.



\section{Profiling}
\note{How do we benchmark?}
\todo{actually find out this!}

We compare the performance overhead of HP, EBR, and without any reclamation. We
look at their performance implication when we operate on the queue and the list,
as described in Section~\ref{sec:data-structures}. We start out by looking at
the performance on a single threaded system, in order to get a notion of how how
much overhead the schemes impose. Then we look at a larger multi threaded system,
so see how well the schemes handle contention. \note{This sounds like a better
fit for the Results chapter?}

\note{Maybe we want to compare before and after comparisons after we have
optimized the code a little? Look at ordering implications?}

\todo{fix ``support testing''} While Rust do support testing, it does not
support benchmarking on the stable release. There exist multiple third party
benchmark suites, most of which are based on the nightly benchmark system. As
the implemented memory reclamation schemes uses global state and thread local
data, we must be careful in how the benchmark suite operates, due to the
problems noted in Section~\ref{sec:thread-cleanup}.  For these reasons, a
separate benchmark crate was developed. The system is simple and captures only
timings of which it reports the average and the variance. These timings were
used to generate the graphs in Chapter~\ref{ch:results}.

We set up multiple benchmarks, in order to try to find different scenarios in
which the schemes performs differently. We have \todo{} different benchmarks:
\begin{description}
  \item[\code{transport}:] We have two queues, one source and one sink. The
    source is pre-filled with \code{N} elements. The threads moves all elements
    from the source to the sink.
\end{description}

An interesting problem that turned up when benchmarking is the following: how do
we benchmark the data structures that does not reclaim any memory? The problem
is that the benchmarking system runs a loop an unspecified number of times, to
make sure the results are statistically significant\note{this is probably not
the right word}. This means that we cannot directly control the number of nodes
we allocate which we do not free, and we risk running out of memory. This causes
the system to swap memory to disk, which destroys performance.

Fortunately, \code{bencher} does support executing code in between $n$ benchmark
iterations, where we get to specify $n$. We can use this to preallocate a
\code{Vec}, in which we put pointers to the memory we allocate in the data
structure we profile. Then, after the $n$ iterations, we free that memory. This
requires some \code{unsafe} code, but it is fully supported by the \code{Vec}
from the standard library. This is a better technique than to allocate the nodes
up front and pass the \code{Node} to the procedures, as this would hide the
allocation cost of the procedure. Listing~\ref{lst:bench-nothing} shows a
benchmark of a Queue using this technique.

\subsection{Single threaded Benchmarks}

\begin{figure}[ht]
  \begin{lstlisting}[caption=Microbenchmark of a data structure without memory
  reclamation,label=lst:bench-nothing]
pub fn push(b: &mut Bencher) {
    const N: u64 = 1024 * 1024;
    b.bench_n(N, |b| {
        let queue = Queue::new();
        let mut ptrs = Vec::with_capacity(N as usize);
        let ptr = ptrs.as_mut_ptr();
        let mut i = 0;
        b.iter(|| {
            queue.push(0usize, unsafe { ptr.offset(i) });
            i += 1;
        });
        unsafe {
            ptrs.set_len(N as usize);
        }
    });
}
  \end{lstlisting}
\end{figure}


\subsection{Multithreaded Benchmarks}


\note{Should add note on microbenchmarks. \code{cargo bench} is alright, but we
get very varying results, since we amortize cleanup over a many runs. This makes
the results given not so good.}


\chapter{Results}\label{ch:results}
\note{What did we run? Plot some graphs. What did we find out? Which scheme is
better for which application? Runtime, power usage, etc.}

We have ran the benchmark suite on five different machines, in order to cover
CPUs from different vendors made for different use cases, hoping to uncover
differences in the reclamation scheme performance. As the CPUs have a very
different core and thread count, the number of threads in each benchmark is not
the same across CPUs, but are capped with the number of hardware threads
supported for the CPU\@. This means that the benchmarks for the lower end CPUs
only use up to 4 threads, while the ones for the highly parallel ARM CPUs use up
to 48. \todo{Double check numbers here.}
The CPUs we have tested on are a
Intel\textregistered{}i7--7500U @ 2.70GHz with 2 cores and 4 threads,
a \todo{mastersal},
a \todo{webkom},
a Ryzen 7 1700 @ 3.0GHz with 8 cores and 16 threads,
and the ARM based Cavium ThunderX @ 2.5GHz with 48 cores and 48 threads.

The benchmakrs are grouped by CPU, each of which have their own section.
Within each section the benchmarks are grouped by operation on a given data
structure. We show box plots for most benchmark results.

It is worth noting that one of the schemes we compare, Crossbeam, is a well
developed open source library, while the remaining, namely EBR, HP and HP-Spin,
are the authors own implementation. It is therefore expected that Crossbeams
offers lower overhead of its operations.


\section{Intel\textregistered{}i7--7500U}
\begin{figure}
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\textwidth]{plots/queue-push-1}
  \end{subfigure}
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\textwidth]{plots/queue-push-2}
  \end{subfigure}
  \\
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\textwidth]{plots/queue-push-4}
  \end{subfigure}
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\textwidth]{plots/queue-push-8}
  \end{subfigure}
  \caption{}
\end{figure}

\begin{figure}
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\textwidth]{plots/queue-pop-1}
  \end{subfigure}
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\textwidth]{plots/queue-pop-2}
  \end{subfigure}
  \\
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\textwidth]{plots/queue-pop-4}
  \end{subfigure}
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\textwidth]{plots/queue-pop-8}
  \end{subfigure}
\end{figure}

\begin{figure}
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\textwidth]{plots/queue-transfer-1}
  \end{subfigure}
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\textwidth]{plots/queue-transfer-2}
  \end{subfigure}
  \\
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\textwidth]{plots/queue-transfer-4}
  \end{subfigure}
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\textwidth]{plots/queue-transfer-8}
  \end{subfigure}
\end{figure}



\section{\todo{mastersal}},
\section{\todo{webkom}}
\section{Ryzen 7 1700}
\section{Cavium ThunderX}


\chapter{Discussion}
\note{Whys. How was Rust? Why are the graphs as they are?
What alternatives exists?}

\begin{appendices}
  \chapter{Rust's Toolchain}
  The Rust toolchain consists of multiple tools. There is the compiler
  \rustc{}, the build tool and dependency manager \cargo{}, and the toolchain
  manager \rustup.  \rustup{} is the preferred way of installing
  Rust\todo{source rustup.rs}.  \rustup{} also handles cross-compiling, i.e.\
  compiling programs for a different architecture.

  A major part of Rust's ecosystem is \url{https://crates.io}, the package repository
  for Rust. This is where packages handled by \cargo{} is downloaded from, by default.

  \section{Hello World}
  We show how to install the Rust toolchain on a Unix based system.
  Installing \rustup{} is done by downloading and running an install script from
  \url{https://rustup.rs}:
  \begin{lstlisting}[language=Bash,numbers=none]
% curl https://sh.rustup.rs -sSf | sh
  \end{lstlisting}
  This installs \rustup{}, \cargo{}, and \rustc{}.
  Next we want to use \cargo{} to make a project. This is done by \code{cargo init}.
  \begin{lstlisting}[language=Bash,numbers=none]
% cargo init --bin <name-of-project>
  \end{lstlisting}
  This will create a directory of the name provided, containing two files:
  \code{Cargo.toml} and \code{src/main.rs}.
  The former is the configuration file of the project, which contains:
  metadata such as the project name, version, authors;
  build options such as optimization levels, debug levels, optional flags;
  and dependencies, with versioning and optional flags.
  The initial \code{Cargo.toml} may look like Listing~\ref{lst:cargo.toml}.
  The other file, \code{src/main.rs} contains the entry point of the program, as
  in Listing~\ref{lst:hello-world}.

  \begin{figure}[p]
  \begin{lstlisting}[language=,basicstyle=\footnotesize\ttfamily,label=lst:cargo.toml,
  caption=A newly generated \code{Cargo.toml}]
[package]
name = "project-name"
version = "0.1.0"
authors = ["Martin Hafskjold Thoresen <martinhath@gmail.com>"]

[dependencies]
  \end{lstlisting}
\end{figure}

\begin{figure}[p]
  \begin{lstlisting}[caption=Hello World in Rust,label=lst:hello-world]
fn main() {
    println!("Hello, world!");
}
  \end{lstlisting}
\end{figure}

  To run the program, we use \cargo{}:

  \begin{figure}[ht]
  \begin{lstlisting}[language=Bash,numbers=none]
% cargo run
   Compiling project-name v0.1.0 (file:///<path>/)
    Finished dev [unoptimized + debuginfo] target(s) in 0.68 secs
     Running `target/debug/project-name`
Hello, world!
%
  \end{lstlisting}
\end{figure}

  Cargo supports a project to build multiple executables, or no executables at all.
  For more information about cargo see~\url{http://doc.crates.io/index.html}.

  \section{\code{\#[test]} and \code{\#[bench]}}
  \label{sec:rust-test}
  The Rust toolchain supports both testing and benchmarking. To write tests we
  make an inline module named \code{test}, and conditionally compile it using
  \code{\#[cfg(test)]}. This makes the code to be ignored unless we are running
  the tests; this is done with \code{cargo test}. Listing~\ref{lst:cargo-test}
  shows an example test on a \code{Queue}. This is usually put in the same file
  at the \code{Queue} but this is only by convention, and not required.

  \begin{figure}[ht!]
  \begin{lstlisting}[label=lst:cargo-test,caption=An example test in Rust]
#[cfg(test)]
mod test {
    use super::*;
    #[test]
    fn queue() {
        let mut queue = Queue::new();
        queue.push(1);
        queue.push(2);
        assert_eq!(queue.pop(), Some(1));
        assert_eq!(queue.pop(), Some(2));
    }
}
    \end{lstlisting}
  \end{figure}
  The output of \code{cargo test} might look like
  Listing~\ref{lst:cargo-test-output}.
  \begin{figure}[ht!]
    \begin{lstlisting}[label=lst:cargo-test-output,caption=Sample output of
    \code{cargo test},language=,numbers=none]
running 47 tests
test ebr::atomic::tests::valid_tag_i8 ... ok
test ebr::atomic::tests::valid_tag_i64 ... ok
test ebr::bench::pin ... ok
test ebr::queue::bench::push ... ok
test ebr::queue::test::can_construct_queue ... ok
test ebr::queue::test::is_unique_receiver ... ok
    \end{lstlisting}
  \end{figure}

  \todo{Replace this with using \code{bencher}.}
  Benchmarking is similar, except that we do not have a conditional compilation
  flag for benchmarks. For similarity, we can put benchmarks in the \code{bench}
  module. Benchmarks are annotated with \code{\#[bench]}. The benchmarks are ran
  with \code{cargo bench}, which runs all benchmark annotated functions.
  Listing~\ref{lst:bench-test} shows a sample benchmark, and
  Listing~\ref{lst:bench-test-output} shows sample output from a benchmark.
  Note that the benchmark system is not in stable Rust, so we need to use the
  nightly version. The code that is benchmarked is the closure passed to
  \code{test::Bencher::iter}.
  \begin{figure}[ht!]
  \begin{lstlisting}[label=lst:bench-test,caption=An example benchmark in Rust]
mod bench {
    extern crate test;
    use super::Queue;
    #[bench]
    fn push(b: &mut test::Bencher) {
        let q = Queue::new();
        b.iter(|| {
            ::ebr::pin(|pin| {
                q.push(1, pin);
            });
        });
    }
}
    \end{lstlisting}
  \end{figure}
  \begin{figure}[ht!]
    \begin{lstlisting}[label=lst:bench-test-output,caption=Sample output of
    \code{cargo bench},language=,numbers=none,basicstyle=\footnotesize]
test ebr::bench::pin               ... bench:   18 ns/iter (+/- 0)
test ebr::queue::bench::push       ... bench:   57 ns/iter (+/- 12)
test hp::list::bench::remove_front ... bench:    2 ns/iter (+/- 0)
    \end{lstlisting}
  \end{figure}





  \section{Setup for cross-compilation}
  Other targets may be installed through \rustup{}. For instance, if we want to
  make \code{aarch64-unknown-linux-gnu} available for cross-compilation we would run
  \begin{lstlisting}[language=Bash,numbers=none]
% rustup target add aarch64-unknown-linux-gnu
  \end{lstlisting}
  We can either pass in the target architecture to \cargo{} at each invocation,
  or we can configure \cargo{} to use another target by default. We show the latter.
  We make a new file in the project directory called \code{.cargo/config}
  containing Listing~\ref{lst:cargo/config}
  \begin{figure}[ht]
  \begin{lstlisting}[language=,
                     basicstyle=\footnotesize\ttfamily,
                     label=lst:cargo/config,
                     caption=Cargo configuration file for cross-compiling]
[build]
target = "aarch64-unknown-linux-gnu"

[target.aarch64-unknown-linux-gnu]
linker = "aarch64-linux-gnu-gcc"
  \end{lstlisting}
  \end{figure}
  This sets the default target to be \code{aarch-unknown-linux-gnu},
  and specifies the linked to be used.
  After this \code{cargo build} builds for the specified target.
  The executable can be found in \code{./target/aarch-unknown-linux-gnu/debug/}
  with the name of the project.



  \todo{Consider add a Git appendix, to show simple checkout stuff, such that
  ``readers'' who don't know Git can clone the repo, and checkout a commit in
  order to run some tests.}

\end{appendices}

\bibliographystyle{acm}
\bibliography{sources}

\end{document}
