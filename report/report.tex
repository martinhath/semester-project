\documentclass[a4paper,twoside]{article}

\usepackage{listings}
\usepackage{color}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{shapes,decorations,shadows}

\begin{document}
\title{Concurrent Memory Reclamation on ARM}
\author{Martin Hafskjold Thoresen}
\date{\today}
\maketitle

\input{macros.tex}

\begin{abstract}
  \todo{update this!}
  Concurrent programs, like any other program, produces garbage.
  In languages that do not have a garbage collector, freeing used
  memory needs to be handeled by the programmer, or through other
  means by the language.
  This report compares 3 different memory reclamation schemes:
  epoch based reclamation, hazard pointers, and reference counting.
  We look at how they perform when used in different common concurrent
  data structures, such as Queues, Lists, and (probably not) Skip-Lists,
  on the ARM platform.
\end{abstract}

\section{Introduction}
\note{Outline: parallelism, garbage collection, what we are about to do,
rust, performance, usability}

\tableofcontents

\section{Background}

\note{I ment to write a quick intro on parallelism, and why we care.
Maybe this is out of scope?}
In 1965 Gordon Moore stated what is now knows as \emph{Moore's Law},
which roughly states that the number of transistors on integrated circuits
will double every 18 months.
The law has also been used to describe the clock speed of central processing
units (CPUs).
However, this exponential growth came to a halt during the 2000s, where
the clock speed of CPUs stopped between 3 GHz and 4 GHz.
Over 10 years later high end consumer CPUs still have a clock
speed in that same range, and the performance increase of modern CPUs
has mainly been due to increase in parallelism\note{has it, though?
Or is it more that CPUs hasn't really improved that much?}.
This improvement however, has not been as noticable as the former improvements in
clock speed. This is because programs already written cannot automatically
use the extra parallelism that newer architectures give us;
Parallelism needs to be accounted for by programmers.

\note{fix this. It reads alright on its own, but the previous paragraph ends
with the same thing, making it very weird to read.}
Despite having multi-core architectures as the most central performance
improvement in CPUs, parallelism is not a success story for programmers.
Most mainstream programming languages does not have first class support
for parallel constructs, and synchronization and work management\note{?} must
still be handeled explicitly by the programmer, using fairly low level
constructs, such as mutexes and threads \note{what are alternatives?}.
\note{what is the motivation behind this? Do we want to make the claim
that Rust is making low level stuff more approachable, and that we can
help improving the performace of programs written by having CMR in Rust?}
In short, parallelism has yet to be approachable for a lot of programmers.
\note{If we are going to complain on the state of parallel programming
we should at least mention some alternatives.}
\note{Parallelism isn't a success story for programmers.
Hard, error-prone, even simple things goes wrong.
Need effective abstractions to improve this. ??}


\subsubsection*{\note{TODO LIST}}
\note{What do we need to understand in order to appreciate this report?
Moores law, parallel applications, multi core, blabla\\
Something about ARM.\\
maybbe notes on state of the art concurrency stuff?
\\
Notes on memory terminology (retire, reclaim, etc.)
\\
Consider adding MS queue or something to show pitfalls in concurrent programming?
}

\subsection{Terminology}
We start with basic terminology of memory management.
Most of the programs we write \emph{allocate} memory, meaning
requesting a memory range from the OS for exlusive use by the program.
This memory is then \emph{deallocated}, or \emph{freed}, when we are done with it,
so that the memory may be used at a later time;
the memory is thus \emph{reclaimed}.
If we want to ``mark'' memory as freed without freeing it (that is, the program
still has exclusive use of it) we say we \emph{retire} the memory
(reasons for doing so will become apparent).

Both modern compilers and CPUs may reorder program instructions
if they think it will improve performace, for instance by
improving memory locality and thus improving cache behaviour,
or reduce pipeline stalling. This may or may not be acceptable by
the program. The guarantee that the instructions executed
appear to take effect in program order is called
\emph{sequential consistency}, and was defined by Leslie Lamport
in 1979\todo{source here}.
We call a system in which threads can be prevented from making
progress \emph{blocking}. For instance, a system using mutexes
is blocking, since a second thread may acquire the mutex,
and be preempted. The oposite of blocking is \emph{non-blocking}.
A method is \emph{lock-free} if we have a guarantee that
\emph{some} thread will make progress.
Note that any given thread may be blocked for an infinite amount of time.
For a more thorough introduction see~\cite{herlihy2011art}.


\note{parallel vs concurrent?}



\subsection{Parallelism in Hardware}
\note{reconsider having hardware and software sections.
Maybe it's better to just have one?}
\note{hardware ordering. Atomic operations (CAS, FA, etc.),
notes on other futuristic atomic operations}
\note{We should focus on the things that are currently supported in hardware,
since this is more of a practical thing.}
We consider multi-core CPUs with multiple levels of cache;
This includes most modern CPUs, both desktop computers and laptops,
as well as mobile devices such as smartphones and tablets.
The first cache level, L1, is usually only used by one core,
while the remaining levels, L2 (and maybe L3), are shared.
This means that when a program is writing to memory,
the same logical memory may be present in multiple
physical locations on the CPU, which causes synchronization
problems when this memory is written to. This is the
problem of \emph{cache coherence}.

\note{???}
We would like our programs to be sequentially consistent.
However, we also want our hardware to be performant, and it
turns out that these two requirements are conflicting, and we have
chosen the latter.

Different CPU architectures have different memory models,
which dicates how much the CPU is allowed to reorder reads and
writes without brekaing the semantics of the program when ran
sequentially.\
x86 have a rahter strong memory model, meaning the CPU is very limited
in its ability to reorder. ARM on the other hand, have a comparably weak
meomry model.
In both arcitechtures we have constructs for saying ``do not move
memory operations across this line'', called \emph{memory fences} or
\emph{barriers}. More details can be found in \todo{find source for this!}.

\note{We should maybe move the Rust section to before this, so we can use
\code{std::sync::atomic::Ordering}}.
The C11 memory model (\note{from where is this actually?}) defined five memory orderings:
Acquire, Release, Acquire+Release, Sequentially Consistent, Relaxed, and
\note{some more? note about unused thing?},
Relaxed is the weakest, with no ordering constraints,
Acquire forbids subsequent operations to be moved before the operation,
Release forbids previous operations to be moved after the operation,
and Sequentially Consistent forbids both.
Acquire+Release is Acquire if the operation is a load,
and Release if it is a store.
A way to remember the acquire and release orderings is that they
hold the semantics of a lock, where we acquire and release the lock:
memory operations inside the critical section is not allowed to be
moved out on either side.




\note{blabalba}





\subsection{Parallelism in Software}
\note{Hardware works such and such, but how does programmers handle these things?
Mention \code{volatile}? Look at instructions?
Many programmers does not think about cache problems. What about
concurrency problems? May they also be ignored? (no)}

Modern compilers \emph{and CPUs} reorder instructions when it improves
performance if they can prove that the end result is the same.
In sequential code there is no way to notice the difference.
Consider Listing~\ref{lst:reordering}, and assume variables
are initialized to 0.
\begin{lstlisting}[caption=Instruction reordering,label=lst:reordering]
// Thread A                     // Thread B
while (f == 0)                  x = 1
  ;                             f = 1
print(x)
\end{lstlisting}
When ran sequentially (alternating between the two), we would observe
no difference if the assignments were reordered. However, when run
concurrently thread B might decide to assign \code{f = 1} first,
and thread A will risk printing \code{0} instead of \code{1},
which was the only case in the sequential world.

We can fix this problem by using a fence.



\subsection{Rust}
% New programming language, memory safety without GC, etc.
\todo{This doesn't read very well. Fix.
This should be written as if you have never heard of Rust,
but if youre fairly familiar with PL terms.
Worst case, we can refer to the PL book if it goes out of hand.}
Rust is a programming language which focus is safety, speed, and concurrency.
It originally emerged from Mozilla Labs in \todo{20??} but is now freely developed
by over \todo{12345} contributors.
Version 1.0 was released in May 2015, and the current stable version is 1.21.
The language is compiled and typed, and features
type inference and virtually no runtime.

The language differs from most other languages in that it features linear types
by its \emph{ownership} semantics.
Values are either \emph{owned} or \emph{borrowed} by its scope.
When an owned value goes out of scope Rust's ownership rules guarantees that
there are no other references to the object and it can be safely \emph{dropped}
by running its destructor when applicable.
This way the programmer does not have to manually manage memory.

\subsubsection{A Crash Course in Rust}

Rust's syntax is based on that of C and OCaml.
Variable bindings are created with the \code{let} construct,
and they are immutable by default, unless marked with \code{mut}.
Functions are declared with \code{fn}, and blocks are surrounded by curly brackets.
The scope operator is \code{::}.

Almost all syntactic constructs in Rust are expressions, including blocks and
\code{if} statements.
Expressions terminated by a semicolon have the value of \emph{unit} (\code{()}).
The return value of a function is the value of its block, which again is the final
expression in the block. Alternatively we can use \code{return}, as in C.
Similiar to C++, Rust programmers prefers \emph{references} over raw pointers.
We use \code{\&} and \code{*} as in C and C++.
As with variables, references are immutable by default; the value they point
to cannot be mutated.

\begin{lstlisting}[firstnumber=last]
// ERROR:
fn add_five(arg: &u32) { *arg += 5; }
// OK:
fn add_five(arg: &mut u32) { *arg += 5; }
\end{lstlisting}

Mutable references cannot alias; that is, it is not allowed to have multiple \code{\&mut}
to the same data, and it is not allowed to have one \code{\&mut} in addition to other
references.

References are guaranteed to be valid, through Rusts usage of \emph{lifetimes}.
A reference is only allowed to live for as long as the data it points to.
This makes it impossible to return a reference to local stack variable in a function:
\begin{lstlisting}[firstnumber=last]
fn add_five() -> &u32 { let n: u32 = 1; &n } // ERROR
\end{lstlisting}

A central point of Rusts philosophy is to build safe abtractions for programmers to use.
For instance, \code{std::box::Box<T>} is a type that heap allocates a value of type \code{T}.
The constructor will allocate the memory, and the destructor will free the memory.
Since references can never outlive the value it references, any other pointer to the value
is guaranteed by the compiler to be valid.
This way we get a safe abtraction over heap allocation.

Through lifetime tracking and the ownership model, Rust avoids the need for a
garbage collector, while still allowing the programmer to not handle memory
management directly. It does however add to the programmers mental overhead,
since lifetime constraints and ownerships of the data must be tracked by their
the mental model.

\todo{Add links to The Book ++}

\todo{A longer code example, comparing Rust to C++17?}

\subsubsection{Examples}
\note{Do we want this?}
\note{look at examples for why the rules in Rust are useful to have enforced by the compiler}

\todo{borrowing}

Assume we have a dynamically sized array (\code{Vec}), and that the exclusive
mutable reference does not exist. Now we may have a reference to a value
in the \code{Vec}, as well as a mutable reference to the \code{Vec} itself.
Consider what happens when we push a new element into the \code{Vec};
the internal buffer may be full, so new memory must be allocated,
the elements moved over, and the old buffer is \code{free}d.
However, this makes the reference to the element invalid, since the memory
it points to is now \code{free}d.

\todo{lifetime}

\subsubsection{Advanced Rust}
\note{Show of why Rust is interesting, eg. how the abstractions can be used,
and why it makes sense to look at Rust.}
\todo{Title of the subchapter}
\todo{Ordering of this stuff}
While Rust features rather strict rules for ownership and lifetimes of all values,
the programmer may bend these rules as they wish.
For instance it is possible to leak a value by not running its destructor,
by using the \code{std::mem::forget} function, or to explicitly run the destrutor of
a value, by calling \code{std::mem::drop}.

Rust also have \emph{raw pointers}, which behaves like regular pointers in C and C++.
Copying and creating raw pointers are safe operations, but reading a raw pointer is
\code{unsafe}, as there are no guarantee on the memory we are reading.

\todo{raw pointers, safe abstractions, some other things.}

\note{Finally we should have some examples of all Rust stuff comming together,
so the reader can see why the language as a whole is interesting.
We could also add some stuff about the asm \code{rustc} generates,
demonstrating ``zero-cost abstractions''.}


\section{Memory Reclamation}
\note{What is memory reclamation? Why is it hard? What approaches are there?}
\note{Assumed background: had OS, have programmed in C or C++.}

When programming we lend memory from the operating system. This memory must be returned, or else
we will sooner or later run out of it.
In most modern programming languages, this is a feature provided by the runtime of the language.
We call such languages \emph{managed languages}.
However, in languages such as Rust, there is virtually no runtime, so this becomes a concern
of the programmer.
Garbage colleciton is typically the name used for memory reclamation;
if we want to pedantic we would say a garbage collector performs memory reclamation.

In a concurrent setting, we are concerned about the properties of our data structures,
\todo{define this}
such as wait-freedom, or lock-freedom.
\todo{rewrite this sentence}
One can make the claim that data structures implemented in managed languages can never
obtain some of these properties, since they depend on the runtime to get execution time
in order for the program to be able to allocate memory.
Hence, the runtime thread has to get CPU time in order for the program to make progress.
\note{Could say this better; the point is to motivate the usage of Rust.}

The field of concurrent memory reclamation is an active one, and a lot of different schemes
have emerged the recent years. We will look at
\emph{Reference Counting} (RC),
\emph{Epoch Based Reclamation} (EBR),
\emph{Hazard Pointers} (HP),
\note{and maybe a few others?}




\subsection{Reference Counting}
\todo{this}
Simple, "obvious" solution. But does it work?

\todo{split into two paragraphs? Eventually move history part to bottom, Knuth style}
The idea of reference counting is that we count the number of references to data,
so that we can tell if we are holding the only reference to some data.
When we no longer need this reference, we know it is safe to reclaim the memory
the reference points to, since no other reference to that memory exists.
Reference counting first appeared in 1960 by G. E. Collins\cite{collins1960method},
where it was used for collecting nodes of a linked list.
The primary downsides of RC is that it is rather expensive, and that a na\"ive
implementation does not reclaim cycles.
Today reference counting is still used, although not in the setting of general
memory reclamation. \todo{source?}
\note{Want to say that while RC isn't used in GC, it is used for other things, like
\code{std::rc::Rc} stuff. Also \code{Arc}, even more expensive}.

% TODO: Use this for node drawing, when needed.
%
% \begin{figure}[ht]
% \begin{tikzpicture}
% \tikzset{Node/.style={
%   rectangle split,
%   rectangle split horizontal,
%   rectangle split parts=2,
%   draw,
%   rounded corners=0.1cm
%   }}
%       \node [Node] (A) at (0,0) {\code{Count=3} \nodepart{second} \code{DATA} };\\
%       \node [Node] (B) at (4,0) {\code{Count} \nodepart{second} \code{DATA} };\\
%       \draw[->]  (0.5,-1) -- (0.5,-0.3);
%       \draw[->]  (0,-1) -- (0,-0.3);
%       \draw[->]  (-0.5,-1) -- (-0.5,-0.3);
% \end{tikzpicture}
%   \caption{Data nodes using RC}
% \end{figure}

\todo{Consider adding a code listing instead of using words}
Reference counting is a natural approach to the problem of concurrent memory reclamation.
The first observation to make is that we need to use atomic reads and writes
to correctly increment and decrement the reference count. However,
the na\"ive implementation is not correct.
Consider two threads operating on some \code{RC<T>}.
When thread A want to create a new reference to the data, it increments the count in
the \code{RC} object. Upon destruction, the count is decremented
and the data is freed if the count is 0.
However, it is possible that thread B has a reference to the RC object
and that it got preempted right before incrementing the count.
Then the whole object gets freed by thread A, since the count is 0,
and when thread B gets execution time again, it has a pointer to freed memory
which it indents to read.
It is worth noting that we can free the \emph{data} using this scheme, but not
the entire \code{RC} node, since the \code{count} field may be set to a special
value indicating that its data is freed.

\note{Add note on differential RC:
http://www.1024cores.net/home/lock-free-algorithms/object-life-time-management/differential-reference-counting}


% TODO:  Code listing sample here, with custom lineno prefix
%
% \begin{lstlisting}[label=lst:rc-broken,
%                    numberblanklines=false,
%                    % TODO: would like to not copy this all over the place
%                    numberstyle=\color{gray}\ttfamily{}RC]
% node = load_node();
% node.count += 1;
% data = node.data;
% // use the node
% node.count -=1;
% if node.count == 0
%   free(node);
% \end{lstlisting}

\subsection{Epoch Based Reclamation}
Epoch Based Reclamation (EBR) was introduced by Fraser in~\cite{fraser2004practical}.
It is a reclamation scheme based on the observation that most programs have no
references to data structure memory in between of operations on the structure.
The time interval in between operations on the data structure are therefore
safepoints (also called grace periods) for memory reclamation to occur.
\todo{add something more here}

EBR uses the concept of an \emph{epoch}, a global timestamp which we
use to find out when it is safe to reclaim retired memory.
The epoch is a global counter.
In addition we have a global list with one entry for each running thread,
which the thread uses for registering the last epoch they read, as well as
whether they are currently performing an operation.
We call a thread performing an operation \emph{pinned}, and the action of
marking and unmarking \emph{pinning} and \emph{unpinning} the thread.

When starting an operation a thread reads the global epoch and saves it
in its entry, and pins the thread.
Upon retiering memory the thread marks the memory with the global epoch
and puts it in a \emph{limbo list} \note{replace with the
three lists instead of marking?}.
Every once in a while, the threads try to increment the epoch.
The epoch can only be incremented if all pinned threads
have seen the current epoch. This way we know that all threads
with references to memory which may be freed by the data structure
is either in the current epoch, or in the previous epoch.
Lastly, after incrementing an epoch to $e$ we know that garbage that
was added in epoch $e-2$ is safe to be freed.
\note{Should clairfy here. Note that the potential counter example
is A a wants to delete X, B incs epoch and reads X,
A deletes X, and incs epoch. But, now X is marked with e+1,
even though A was in e, since we're reading the global epoch on delete.}

\note{Add example from comere?}

There are still a few challenges with EBR.
A problem is that we are not allowed to keep references to data across operations,
since the thread must be pinned while we are using the references.
A natural way to mitigate this constraint is to leave the thread pinned.
However, this will stop the advancement of the global epoch, and thus effectively
halting the memory reclamation.
An immediate consequence of this is that EBR is not lock-free.
\note{Limits/challenges of EBR. Add more.}

\todo{Add proof? Pseudocode of stuff?}


\subsection{Hazard Pointers}

Hazard pointers was introduced by Michael in~\cite{michael2004hazard}.
It is based on the observation that in most operations on data structure
we only need a small constant number of references to memory that is shared
between running threads. The technique exploits this by allowing each
thread to reigster the pointers, called \emph{hazard pointers},
the thread is using at a given instant. This way other threads can tell
if memory they are operating on is used by other threads.
The number of poiters we need varies with the algorithms performed,
but a typical value is one or two.

\note{This doesn't really make sense - both techniques requires
some manual work, be it pointer registration or pinning stuff.
Also, while this might be a difference in the two schemes as a whole,
it is probably more interesting to take the comparison in the implementation part.}
A big difference in usage of HP from EBR is that identifying which
pointers we need to register is a task which cannot be automated.
We need to manually go though our algorithms and look for pointer
usage which may fail, and add registration code inline.
This is of great contrast to EBR, in which it was possible to
just pin and unpin the thread before and after operations.


\todo{Revisit RC here, with the stuff from the blog?
Or mabye its better to just put it at the end of the subsection.}

\subsection{Other Schemes}
\note{We don't have time to implement all interesting schemes,
but it would be nice to have them in the report}


\section{Implementation}
\note{How did we implement the schemes? What choices did we have to take?
Whys. Also implementation problems, testing techniques, profiling setup etc.}

\todo{Rename to methology?}


\section{Results}
\note{What did we run? Plot some graphs. What did we find out?
Which scheme is better for which application?
Runtime, power usage, etc.}


\section{Discussion}
\note{Whys. How was Rust? Why are the graphs as they are?
What alternatives exists?}

\bibliographystyle{acm}
\bibliography{sources}

\end{document}
